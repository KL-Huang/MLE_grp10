{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col, lower, trim, when,row_number, count\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import utils.data_processing_bronze_table as bronze_processing\n",
    "import utils.data_processing_silver_table as silver_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a822439-ea37-4662-a83e-1a4d48b1ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---starting job---\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/17 20:22:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n---starting job---\\n\\n')\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"olist_bronze_processing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cddd1-b87b-42dd-8158-8ecf9bd6b839",
   "metadata": {},
   "source": [
    "## Build Bronze Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e56e7-671b-4582-a4a7-9d551e83a226",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the bronze tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the bronze tables there.\n",
    "\n",
    "Need to have team meeting to resolve this\n",
    "\n",
    "I chose to run the main.py script, therefore subsequent code on Silver Tables built references the path from `app` folder to access the bronze tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b6d22e-b746-4247-80a9-ecdd471b7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze root directory: datamart/bronze\n"
     ]
    }
   ],
   "source": [
    "# Create bronze root directory\n",
    "bronze_root = \"datamart/bronze\"\n",
    "os.makedirs(bronze_root, exist_ok=True)\n",
    "print(f\"Bronze root directory: {bronze_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20351f49-7a7b-4556-94af-db3b92c37668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Olist datasets...\n",
      "\n",
      "loaded data/olist_customers_dataset.csv  →  99,441 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bronze: datamart/bronze/customers/bronze_olist_customers.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_geolocation_dataset.csv  →  1,000,325 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bronze: datamart/bronze/geolocation/bronze_olist_geolocation.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_items_dataset.csv  →  112,650 rows\n",
      "saved bronze: datamart/bronze/order_items/bronze_olist_order_items.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_payments_dataset.csv  →  103,886 rows\n",
      "saved bronze: datamart/bronze/order_payments/bronze_olist_order_payments.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_reviews_dataset.csv  →  104,162 rows\n",
      "saved bronze: datamart/bronze/order_reviews/bronze_olist_order_reviews.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_products_dataset.csv  →  32,951 rows\n",
      "saved bronze: datamart/bronze/products/bronze_olist_products.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_sellers_dataset.csv  →  3,095 rows\n",
      "saved bronze: datamart/bronze/sellers/bronze_olist_sellers.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/product_category_name_translation.csv  →  71 rows\n",
      "saved bronze: datamart/bronze/category_translation/bronze_product_category_translation.parquet\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process all Olist datasets\n",
    "print(\"\\nProcessing Olist datasets...\\n\")\n",
    "bronze_processing.process_olist_customers_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_geolocation_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_items_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_payments_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_reviews_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_products_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_sellers_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_product_cat_translation_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10db75df-e10c-477e-9433-c107cf8963bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-04: 6939 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-02: 6728 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-01: 7269 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-12: 5673 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-05: 3700 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_05.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-07: 6292 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-11: 7544 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-01: 800 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2016-09: 4 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2016-10: 324 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-03: 2682 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-04: 2404 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-10: 4631 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-09: 4285 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-02: 1780 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_02.csv\n",
      "Month 2018-09: 16 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-06: 3245 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-06: 6167 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-07: 4026 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-08: 6512 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_08.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-08: 4331 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_08.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-03: 7211 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-05: 6873 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_05.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-10: 4 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2016-12: 1 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_12.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: string, customer_id: string, order_status: string, order_purchase_timestamp: timestamp, order_approved_at: timestamp, order_delivered_carrier_date: timestamp, order_delivered_customer_date: timestamp, order_estimated_delivery_date: timestamp, snapshot_date: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process orders with monthly partitioning\n",
    "bronze_processing.process_olist_orders_bronze(bronze_root, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e42811-8a18-4f1d-bf62-7108dc267e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|503840d4f2a1a7609...|ffc4233210eac4ec1...|                   14811|          araraquara|            SP|\n",
      "|52e73a5d0a1d4c56b...|b43530186123fb6d9...|                   62625|               missi|            CE|\n",
      "|16cb62869f9719571...|c3cc321141423ab8a...|                   55560|           barreiros|            PE|\n",
      "|4979ba0e6037e4b28...|80768413a59684f1e...|                   29307|cachoeiro de itap...|            ES|\n",
      "|11ec4bc0610184925...|bd836cf4fce7f808b...|                   22420|      rio de janeiro|            RJ|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "# I put the actual path due to the discrepancy in paths above. Will amend later\n",
    "# df_bronze = spark.read.parquet(\"../datamart/bronze/customers/bronze_olist_customers.parquet\")\n",
    "# df_bronze.show(5)\n",
    "\n",
    "# Can read\n",
    "\n",
    "\n",
    "# Maanoj for testing\n",
    "df_bronze = spark.read.parquet(\"datamart/bronze/customers/bronze_olist_customers.parquet\")\n",
    "df_bronze.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767bf08-b027-43bd-a695-43b5217fd756",
   "metadata": {},
   "source": [
    "## Build Silver Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d6071-2d84-491f-81d9-a2f0f692e526",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the silver tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the silver tables there.\n",
    "\n",
    "Need to have team meeting to resolve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2b29d3-9ac3-4246-a52f-d67fadf64940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver root directory: datamart/silver\n"
     ]
    }
   ],
   "source": [
    "# Create silver root directory\n",
    "silver_root = \"datamart/silver\"\n",
    "os.makedirs(silver_root, exist_ok=True)\n",
    "print(f\"Silver root directory: {silver_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1012f779-1308-4a32-9e1e-e6bfc9289cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all required output directories\n",
    "\n",
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)\n",
    "\n",
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)\n",
    "\n",
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)\n",
    "\n",
    "\n",
    "# Create silver directory to save products data\n",
    "silver_prod_directory = \"datamart/silver/products/\"\n",
    "if not os.path.exists(silver_prod_directory):\n",
    "    os.makedirs(silver_prod_directory)\n",
    "\n",
    "# # Create silver directory to save product_categories data\n",
    "# silver_prod_cat_directory = \"datamart/silver/product_categories/\"\n",
    "# if not os.path.exists(silver_prod_cat_directory):\n",
    "#     os.makedirs(silver_prod_cat_directory)\n",
    "\n",
    "# Create silver directory to save orders data\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "if not os.path.exists(silver_orders_directory):\n",
    "    os.makedirs(silver_orders_directory)\n",
    "\n",
    "# Create silver directory to save order_items data\n",
    "silver_order_items_directory = \"datamart/silver/order_items/\"\n",
    "if not os.path.exists(silver_order_items_directory):\n",
    "    os.makedirs(silver_order_items_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4dee92-5661-46b9-85ad-e84d0448147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing bronze tables...\n",
      "loaded from: ../datamart/bronze/customers/bronze_olist_customers.parquet row count: 99441\n",
      "Number of duplicated 'customer_id': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/customers/silver_olist_customers.parquet\n",
      "loaded from: ../datamart/bronze/sellers/bronze_olist_sellers.parquet row count: 3095\n",
      "Number of duplicated 'seller_id': 0\n",
      "saved to: datamart/silver/sellers/silver_olist_sellers.parquet\n",
      "loaded from: ../datamart/bronze/geolocation/bronze_olist_geolocation.parquet row count: 1000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/geolocation/silver_olist_geolocation.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[geolocation_zip_code_prefix: string, geolocation_lat: double, geolocation_lng: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all bronze tables into silver\n",
    "print(\"\\nProcessing bronze tables...\")\n",
    "silver_processing.process_silver_olist_customers(\"../datamart/bronze/customers/\",silver_cust_directory, spark)\n",
    "silver_processing.process_silver_olist_sellers(\"../datamart/bronze/sellers/\",silver_sell_directory, spark)\n",
    "silver_processing.process_silver_olist_geolocation(\"../datamart/bronze/geolocation/\",silver_geo_directory, spark)\n",
    "\n",
    "# add more below\n",
    "silver_processing.process_silver_olist_products(\"../datamart/bronze/products/\",silver_prod_directory, spark)\n",
    "# silver_processing.process_silver_olist_product_categories(\"../datamart/bronze/??/\",silver_prod_cat_directory, spark)\n",
    "silver_processing.process_silver_olist_orders(\"../datamart/bronze/orders/\",silver_orders_directory, spark)\n",
    "silver_processing.process_silver_olist_order_items(\"../datamart/bronze/order_items/\",silver_order_items_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735fc5a-0b84-4d4d-9a39-5fcf13b359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process orders with monthly partitioning\n",
    "# add more below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9692dab1-3c00-427d-9abf-9319dadf04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "|                      49290|-11.274805005391439|-37.790795516967776|\n",
      "|                      49630|-10.605308055877686|-37.113027572631836|\n",
      "|                      55445|   -8.5616774559021|  -35.8295783996582|\n",
      "|                      57051| -9.655002400681779| -35.73440123893119|\n",
      "|                      57085| -9.558634171119103| -35.73914117079515|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df_silver = spark.read.parquet(\"datamart/silver/geolocation/silver_olist_geolocation.parquet\")\n",
    "df_silver.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b00aa2-e29a-4965-866f-a1f593ab4c23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Customer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81fd1733-5963-4536-987b-528e045dba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c4513d-86f0-4807-9cc4-eb1d3a694448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_customers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_customers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"customer_id\": StringType(),\n",
    "        \"customer_unique_id\": StringType(),\n",
    "        \"customer_zip_code_prefix\": StringType(),\n",
    "        \"customer_city\": StringType(),\n",
    "        \"customer_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check customer_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"customer_id\").distinct().count()\n",
    "    duplicates_customer_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'customer_id': {duplicates_customer_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"customer_zip_code_prefix\",\n",
    "        F.lpad(col(\"customer_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_customers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14ca153f-67d1-412a-a729-7080f9bc5e5f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/app/datamart/bronze/customers/bronze_olist_customers.parquet. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAnalysisException\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run function manually to test\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mprocess_silver_olist_customers\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m../datamart/bronze/customers/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43msilver_cust_directory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspark\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mprocess_silver_olist_customers\u001b[39m\u001b[34m(bronze_directory, silver_directory, spark)\u001b[39m\n\u001b[32m      4\u001b[39m partition_name = \u001b[33m\"\u001b[39m\u001b[33mbronze_olist_customers.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m filepath = bronze_directory + partition_name\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m df = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mloaded from:\u001b[39m\u001b[33m'\u001b[39m, filepath, \u001b[33m'\u001b[39m\u001b[33mrow count:\u001b[39m\u001b[33m'\u001b[39m, df.count())\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# clean data: enforce schema / data type\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Dictionary specifying columns and their desired datatypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/sql/readwriter.py:642\u001b[39m, in \u001b[36mDataFrameReader.parquet\u001b[39m\u001b[34m(self, *paths, **options)\u001b[39m\n\u001b[32m    631\u001b[39m int96RebaseMode = options.get(\u001b[33m\"\u001b[39m\u001b[33mint96RebaseMode\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    632\u001b[39m \u001b[38;5;28mself\u001b[39m._set_opts(\n\u001b[32m    633\u001b[39m     mergeSchema=mergeSchema,\n\u001b[32m    634\u001b[39m     pathGlobFilter=pathGlobFilter,\n\u001b[32m   (...)\u001b[39m\u001b[32m    639\u001b[39m     int96RebaseMode=int96RebaseMode,\n\u001b[32m    640\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jreader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_spark\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/py4j/java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.12/site-packages/pyspark/errors/exceptions/captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mAnalysisException\u001b[39m: [PATH_NOT_FOUND] Path does not exist: file:/app/datamart/bronze/customers/bronze_olist_customers.parquet. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_customers(\"../datamart/bronze/customers/\",silver_cust_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f8aef-d715-44d9-9f81-c8c7202432b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f823b48-1e8c-4b5c-a38e-0921dbae1c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"customer_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f0173-1008-4f1c-a776-f36daec1746b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Seller Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12ae85-298f-454d-8a35-daddd3897a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bea31b9-621e-4630-8b17-1106edf023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_sellers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_sellers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"seller_id\": StringType(),\n",
    "        \"seller_zip_code_prefix\": StringType(),\n",
    "        \"seller_city\": StringType(),\n",
    "        \"seller_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check seller_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"seller_id\").distinct().count()\n",
    "    duplicates_seller_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'seller_id': {duplicates_seller_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"seller_zip_code_prefix\",\n",
    "        F.lpad(col(\"seller_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_sellers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3ef983-9e83-4970-85fd-d7358d2f3de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_sellers(\"../datamart/bronze/sellers/\",silver_sell_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1468622b-78f2-4fd7-a0b9-6c2e4b5a6cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066674d2-760b-47fe-89ef-0be33600f1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"seller_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73580bff-eb08-412b-b91e-59ed8767181a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Geolocation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8548b10-ed7f-41d6-b767-f23f4c771a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437f8bf-83cd-475d-a948-4bffd41e2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_geolocation(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_geolocation.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"geolocation_zip_code_prefix\": StringType(),\n",
    "        \"geolocation_lat\": FloatType(),\n",
    "        \"geolocation_lng\": FloatType(),\n",
    "        \"geolocation_city\": StringType(),\n",
    "        \"geolocation_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"geolocation_zip_code_prefix\",\n",
    "        F.lpad(col(\"geolocation_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "\n",
    "    # Deduplicate zipcodes by just taking the centroid (mean of lat,lng)\n",
    "    df_dedupe = df.groupBy(\"geolocation_zip_code_prefix\").agg(\n",
    "        F.avg(\"geolocation_lat\").alias(\"geolocation_lat\"),\n",
    "        F.avg(\"geolocation_lng\").alias(\"geolocation_lng\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_geolocation.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744285be-254a-482c-bbd4-f1b4dc73cef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_silver_olist_geolocation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run function manually to test\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mprocess_silver_olist_geolocation\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m../datamart/bronze/geolocation/\u001b[39m\u001b[33m\"\u001b[39m,silver_geo_directory, spark)\n",
      "\u001b[31mNameError\u001b[39m: name 'process_silver_olist_geolocation' is not defined"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_geolocation(\"../datamart/bronze/geolocation/\",silver_geo_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a145539-6949-49b3-bc73-f95e92a0191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geolocation_zip_code_prefix: string (nullable = true)\n",
      " |-- geolocation_lat: double (nullable = true)\n",
      " |-- geolocation_lng: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a998b9-e0d1-46df-90f8-7ac1fc45901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|19177|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"geolocation_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5857ffa-e899-4d38-a856-92d3a9c2a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----+\n",
      "|geolocation_zip_code_prefix|count|\n",
      "+---------------------------+-----+\n",
      "+---------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check every geolocation_zip_code_prefix only has 1 count. Group by prefix and count occurrences\n",
    "df.groupBy(\"geolocation_zip_code_prefix\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .filter(\"count > 1\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc538d2b-2b37-478b-844a-7af3724f1dcb",
   "metadata": {},
   "source": [
    "### Build Products Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4e4909e9-3347-4b1f-9728-aba795fc18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save products data\n",
    "silver_prod_directory = \"datamart/silver/products/\"\n",
    "if not os.path.exists(silver_prod_directory):\n",
    "    os.makedirs(silver_prod_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a42f5caf-d676-4975-b67c-203483680798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_products(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_products.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # Rename columns due to spelling mistakes \n",
    "    df = df.withColumnRenamed(\"product_name_lenght\", \"product_name_length\") \\\n",
    "           .withColumnRenamed(\"product_description_lenght\", \"product_description_length\")\n",
    "\n",
    "    \n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"product_id\": StringType(),\n",
    "        \"product_category_name\": StringType(),\n",
    "        \"product_name_length\": DoubleType(),\n",
    "        \"product_description_length\": DoubleType(),\n",
    "        \"product_photos_qty\": DoubleType(),\n",
    "        \"product_weight_g\": DoubleType(),\n",
    "        \"product_length_cm\": DoubleType(),\n",
    "        \"product_height_cm\": DoubleType(),\n",
    "        \"product_width_cm\": DoubleType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Inputting missing values as NaN\n",
    "    df = df.fillna({\"product_category_name\": \"NaN\"})\n",
    "    df = df.fillna({\"product_name_length\": float('nan')}) \n",
    "    df = df.fillna({\"product_description_length\": float('nan')}) \n",
    "    df = df.fillna({\"product_photos_qty\": float('nan')}) \n",
    "    \n",
    "    # Check product_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"product_id\").distinct().count()\n",
    "    duplicates_product_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'product_id': {duplicates_product_id}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_products.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02873004-1348-4cf8-9323-cf9a4692b4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/products/bronze_olist_products.parquet row count: 32951\n",
      "Number of duplicated 'product_id': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/products/silver_olist_products.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_products(\"datamart/bronze/products/\",silver_prod_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3864036-80cd-4e10-b1e8-9262992cd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_category_name: string (nullable = false)\n",
      " |-- product_name_length: double (nullable = false)\n",
      " |-- product_description_length: double (nullable = false)\n",
      " |-- product_photos_qty: double (nullable = false)\n",
      " |-- product_weight_g: double (nullable = true)\n",
      " |-- product_length_cm: double (nullable = true)\n",
      " |-- product_height_cm: double (nullable = true)\n",
      " |-- product_width_cm: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "62d0c758-2904-4604-83eb-9fa01659f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|          product_id|product_category_name|product_name_length|product_description_length|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|1e9e8ef04dbcff454...|           perfumaria|               40.0|                     287.0|               1.0|           225.0|             16.0|             10.0|            14.0|\n",
      "|3aa071139cb16b67c...|                artes|               44.0|                     276.0|               1.0|          1000.0|             30.0|             18.0|            20.0|\n",
      "|96bd76ec8810374ed...|        esporte_lazer|               46.0|                     250.0|               1.0|           154.0|             18.0|              9.0|            15.0|\n",
      "|cef67bcfe19066a93...|                bebes|               27.0|                     261.0|               1.0|           371.0|             26.0|              4.0|            26.0|\n",
      "|9dc1a7de274444849...| utilidades_domest...|               37.0|                     402.0|               4.0|           625.0|             20.0|             17.0|            13.0|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df = spark.read.parquet(\"datamart/silver/products/silver_olist_products.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e4638-eb87-4613-9279-a740b36a712a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Product Categories Table??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "761af31e-a1ce-48f7-b8c6-2458611a1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddad9d-b277-435c-bf4b-2cb863a93f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fcfbe42-badc-42ee-9b92-bf2b9a665a79",
   "metadata": {},
   "source": [
    "### Build Orders Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dd19e8c-2ac8-4c03-9917-2c7073ef2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save orders data\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "if not os.path.exists(silver_orders_directory):\n",
    "    os.makedirs(silver_orders_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "821674c0-c4ed-415b-96a5-fef4be96d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_orders(bronze_directory, silver_directory, spark, partition_name):\n",
    "    filepath = os.path.join(bronze_directory, partition_name)\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "\n",
    "# def process_silver_olist_orders(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    \n",
    "    # partition_name = \"bronze_olist_orders.parquet\"\n",
    "    # filepath = bronze_directory + partition_name\n",
    "    # df = spark.read.parquet(filepath)\n",
    "    # print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"order_id\": StringType(),\n",
    "        \"customer_id\": StringType(),\n",
    "        \"order_status\": StringType(),\n",
    "        \"order_purchase_timestamp\": TimestampType(),\n",
    "        \"order_approved_at\": TimestampType(),\n",
    "        \"order_delivered_carrier_date\": TimestampType(),\n",
    "        \"order_delivered_customer_date\": TimestampType(),\n",
    "        \"order_estimated_delivery_date\": TimestampType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Removing Invalid order ids\n",
    "    # Load the Bronze table \n",
    "    df_order_items = spark.read.parquet(\"datamart/bronze/order_items/bronze_olist_order_items.parquet\") \n",
    "    \n",
    "    # Get distinct order IDs that exist in order items\n",
    "    valid_order_ids_df = df_order_items.select(\"order_id\").distinct()\n",
    "    \n",
    "    \n",
    "    # Keep only orders that exist in df_order_items\n",
    "    df_orders_clean = df.join(valid_order_ids_df, on=\"order_id\", how=\"inner\")\n",
    "    \n",
    "    # Count how many were dropped\n",
    "    dropped_orders = df.count() - df_orders_clean.count()\n",
    "    print(f\"Dropped {dropped_orders} orders with no items.\")\n",
    "\n",
    "    df = df_orders_clean\n",
    "\n",
    "\n",
    "    # Checking for invalid customer IDs\n",
    "    # Load df_customers from Bronze\n",
    "    df_customers = spark.read.parquet(\"datamart/bronze/customers/bronze_olist_customers.parquet\")  \n",
    "\n",
    "    # Get distinct valid customer IDs\n",
    "    valid_customer_ids_df = df_customers.select(\"customer_id\").distinct()\n",
    "    \n",
    "    # Perform a left anti join to find orders with invalid customer_id\n",
    "    invalid_orders = df.join(valid_customer_ids_df, on=\"customer_id\", how=\"left_anti\")\n",
    "    \n",
    "    # Count how many invalid customer IDs there are\n",
    "    invalid_customer_count = invalid_orders.count()\n",
    "\n",
    "    # Conditionally drop invalid orders\n",
    "    if invalid_customer_count > 0:\n",
    "        initial_count = df.count()\n",
    "        print(\"Dropping orders with invalid customer_id...\")\n",
    "        df = df.join(valid_customer_ids_df, on=\"customer_id\", how=\"inner\")\n",
    "        final_count = df.count()\n",
    "        dropped_count = initial_count - final_count\n",
    "        print(f\"Dropped {dropped_count} rows\")\n",
    "        \n",
    "    else:\n",
    "        print(\"All customer ids are valid — no need to drop!!\")\n",
    "\n",
    "\n",
    "\n",
    "    # Enforcing enum for order statuses\n",
    "    # Define valid statuses \n",
    "    valid_statuses = {\n",
    "        \"created\",\n",
    "        \"approved\",\n",
    "        \"processing\",\n",
    "        \"invoiced\",\n",
    "        \"shipped\",\n",
    "        \"delivered\",\n",
    "        \"canceled\",\n",
    "        \"unavailable\"\n",
    "    }\n",
    "    \n",
    "    # Clean and standardize the `order_status` column\n",
    "    df = df.withColumn(\"order_status\", trim(lower(col(\"order_status\"))))\n",
    "    \n",
    "    # dentify invalid statuses (those NOT in the valid_statuses set)\n",
    "    invalid_statuses_df = df.filter(~col(\"order_status\").isin(list(valid_statuses)))\n",
    "    \n",
    "    # Print the unique invalid statuses\n",
    "    invalid_statuses_list = invalid_statuses_df.select(\"order_status\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    if invalid_statuses_list:\n",
    "        print(f\"Invalid statuses found: {invalid_statuses_list}\")\n",
    "    else:\n",
    "        print(\"No invalid status found!!\")\n",
    "\n",
    "\n",
    "    \n",
    "    # # save silver table - IRL connect to database to write\n",
    "    # partition_name = \"silver_olist_orders_2016_09.parquet\"  \n",
    "    # filepath = silver_directory + partition_name\n",
    "    # df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    # print('saved to:', filepath)\n",
    "\n",
    "\n",
    "    # save \n",
    "    parquet_name = partition_name.replace(\"bronze\", \"silver\").replace(\".csv\", \".parquet\")\n",
    "    output_path = os.path.join(silver_directory, parquet_name)\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(\"-----> saved to:\", output_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1c38f543-33b9-4ae0-ab4c-c34776d9ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Processing bronze_olist_orders_2016_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_09.csv row count: 4\n",
      "Dropped 1 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2016_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2016_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_10.csv row count: 324\n",
      "Dropped 16 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2016_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2016_12.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_12.csv row count: 1\n",
      "Dropped 0 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2016_12.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_01.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_01.csv row count: 800\n",
      "Dropped 11 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_01.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_02.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_02.csv row count: 1780\n",
      "Dropped 47 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_02.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_03.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_03.csv row count: 2682\n",
      "Dropped 41 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_03.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_04.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_04.csv row count: 2404\n",
      "Dropped 13 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_04.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_05.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_05.csv row count: 3700\n",
      "Dropped 40 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_05.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_06.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_06.csv row count: 3245\n",
      "Dropped 28 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_06.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_07.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_07.csv row count: 4026\n",
      "Dropped 57 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_07.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_08.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_08.csv row count: 4331\n",
      "Dropped 38 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_08.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_09.csv row count: 4285\n",
      "Dropped 42 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_10.csv row count: 4631\n",
      "Dropped 63 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_11.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_11.csv row count: 7544\n",
      "Dropped 93 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_11.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_12.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_12.csv row count: 5673\n",
      "Dropped 49 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_12.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_01.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_01.csv row count: 7269\n",
      "Dropped 49 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_01.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_02.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_02.csv row count: 6728\n",
      "Dropped 34 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_02.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_03.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_03.csv row count: 7211\n",
      "Dropped 23 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_03.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_04.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_04.csv row count: 6939\n",
      "Dropped 5 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_04.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_05.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_05.csv row count: 6873\n",
      "Dropped 20 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_05.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_06.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_06.csv row count: 6167\n",
      "Dropped 7 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_06.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_07.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_07.csv row count: 6292\n",
      "Dropped 19 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_07.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_08.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_08.csv row count: 6512\n",
      "Dropped 60 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_08.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_09.csv row count: 16\n",
      "Dropped 15 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_10.csv row count: 4\n",
      "Dropped 4 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "\n",
    "# Set base directory\n",
    "bronze_orders_directory = \"datamart/bronze/orders/\"\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "\n",
    "# List all CSV files in the bronze orders folder\n",
    "csv_files = [f for f in os.listdir(bronze_orders_directory) if f.endswith(\".csv\")]\n",
    "\n",
    "# Sort the files according to date\n",
    "csv_files.sort()\n",
    "\n",
    "# Loop through each file \n",
    "for partition_name in csv_files:\n",
    "    print(f\"\\n======== Processing {partition_name} ......... \\n\")\n",
    "    df = process_silver_olist_orders(bronze_orders_directory, silver_orders_directory, spark, partition_name)\n",
    "    \n",
    "    # Check schema enforced\n",
    "    df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57dda5c-b11c-4bcc-833c-89b9158b35f7",
   "metadata": {},
   "source": [
    "### Build Order_Items Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "950f9874-fa4c-47c1-823e-d9e6d21104e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save order_items data\n",
    "silver_order_items_directory = \"datamart/silver/order_items/\"\n",
    "if not os.path.exists(silver_order_items_directory):\n",
    "    os.makedirs(silver_order_items_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a05472f-0707-49c9-a85a-7a8addb30c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_order_items(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_order_items.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    \n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"order_id\": StringType(),\n",
    "        \"order_item_id\": LongType(),\n",
    "        \"product_id\": StringType(),\n",
    "        \"seller_id\": StringType(),\n",
    "        \"shipping_limit_date\": TimestampType(),\n",
    "        \"price\": DoubleType(),\n",
    "        \"freight_value\": DoubleType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    \n",
    "    # Checking for invalid seller IDs\n",
    "    # Load df_sellers from Bronze\n",
    "    df_sellers = spark.read.parquet(\"datamart/bronze/sellers/bronze_olist_sellers.parquet\")  \n",
    "\n",
    "    # Get distinct valid seller IDs\n",
    "    valid_seller_ids_df = df_sellers.select(\"seller_id\").distinct()\n",
    "    \n",
    "    # Perform a left anti join to find sellers with invalid seller_id\n",
    "    invalid_orders = df.join(valid_seller_ids_df, on=\"seller_id\", how=\"left_anti\")\n",
    "    \n",
    "    # Count how many invalid seller IDs there are\n",
    "    invalid_seller_count = invalid_orders.count()\n",
    "\n",
    "    # Conditionally drop invalid orders\n",
    "    if invalid_seller_count > 0:\n",
    "        initial_count = df.count()\n",
    "        print(\"Dropping orders with invalid seller_id...\")\n",
    "        df = df.join(valid_seller_ids_df, on=\"seller_id\", how=\"inner\")\n",
    "        final_count = df.count()\n",
    "        dropped_count = initial_count - final_count\n",
    "        print(f\"Dropped {dropped_count} rows\")\n",
    "        \n",
    "    else:\n",
    "        print(\"All seller ids are valid — no need to drop!!\")\n",
    "\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_order_items.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5b7b5663-92f0-426e-b6a8-9e7faa45d77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/order_items/bronze_olist_order_items.parquet row count: 112650\n",
      "All seller ids are valid — no need to drop!!\n",
      "saved to: datamart/silver/order_items/silver_olist_order_items.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_order_items(\"datamart/bronze/order_items/\",silver_order_items_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff299665-0d75-468e-acc7-15fd8f412f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "97a8e4aa-0994-4877-9981-a7927e3febbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|89bed55dd88d035e3...|            1|ae6b739ab6e9d7991...|53e4c6e0f4312d4d2...|2018-01-11 17:12:22|  42.0|         17.6|\n",
      "|89c037e2b749a2ed5...|            1|e906fa76a27488f80...|1835b56ce799e6a4d...|2017-11-30 04:15:33| 53.99|        12.72|\n",
      "|89c037e2b749a2ed5...|            2|e906fa76a27488f80...|1835b56ce799e6a4d...|2017-11-30 04:15:33| 53.99|        12.72|\n",
      "|89c04d22504649482...|            1|3d73c88390adac7dd...|c8b0e2b0a7095e5d8...|2018-03-14 00:20:26|199.99|        25.05|\n",
      "|89c0bf5292a493fb2...|            1|2d40d83fc97b8d4d4...|cca3071e3e9bb7d12...|2017-08-24 03:26:15|  29.9|        11.85|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df = spark.read.parquet(\"datamart/silver/order_items/silver_olist_order_items.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff30d78-c8f6-4af9-bb89-57c326a8cb92",
   "metadata": {},
   "source": [
    "### Building Derived Table - Order logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b0d2287-aae6-410f-af54-4dd4f61863ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_order_features(spark, order_file_path):\n",
    "    # Read inputs\n",
    "    df_order_items = spark.read.parquet(\"datamart/silver/order_items/silver_olist_order_items.parquet\")\n",
    "    df_products = spark.read.parquet(\"datamart/silver/products/silver_olist_products.parquet\")\n",
    "    df_categories = spark.read.parquet(\"datamart/bronze/category_translation/bronze_product_category_translation.parquet\")\n",
    "    df_orders = spark.read.parquet(order_file_path)\n",
    "\n",
    "    order_metrics = df_order_items.groupBy(\"order_id\").agg(\n",
    "        F.max(\"order_item_id\").alias(\"total_qty\"),\n",
    "        F.sum(\"price\").alias(\"total_price\"),\n",
    "        F.sum(\"freight_value\").alias(\"total_freight_value\")\n",
    "    )\n",
    "\n",
    "    df_items_with_products = df_order_items.select(\"order_id\", \"product_id\") \\\n",
    "        .join(\n",
    "            df_products.select(\n",
    "                \"product_id\", \"product_weight_g\",\n",
    "                \"product_length_cm\", \"product_height_cm\", \"product_width_cm\"\n",
    "            ),\n",
    "            on=\"product_id\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    df_items_with_products = df_items_with_products.withColumn(\n",
    "        \"product_volume_cm3\",\n",
    "        col(\"product_length_cm\") * col(\"product_height_cm\") * col(\"product_width_cm\")\n",
    "    )\n",
    "\n",
    "    product_metrics = df_items_with_products.groupBy(\"order_id\").agg(\n",
    "        F.sum(\"product_weight_g\").alias(\"total_weight_g\"),\n",
    "        F.sum(\"product_volume_cm3\").alias(\"total_volume_cm3\")\n",
    "    )\n",
    "\n",
    "    final_df = df_orders.select(\"order_id\", \"order_purchase_timestamp\") \\\n",
    "        .join(order_metrics, on=\"order_id\", how=\"inner\") \\\n",
    "        .join(product_metrics, on=\"order_id\", how=\"left\") \\\n",
    "        .withColumn(\n",
    "            \"total_density\",\n",
    "            when(col(\"total_volume_cm3\") != 0,\n",
    "                 col(\"total_weight_g\") / col(\"total_volume_cm3\")\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "\n",
    "    df_items_with_cats = df_order_items.select(\"order_id\", \"product_id\") \\\n",
    "        .join(df_products.select(\"product_id\", \"product_category_name\"), on=\"product_id\", how=\"left\") \\\n",
    "        .join(df_categories.select(\"product_category_name\", \"main_category\", \"sub_category\"), on=\"product_category_name\", how=\"left\")\n",
    "\n",
    "    main_cat_counts = df_items_with_cats.groupBy(\"order_id\", \"main_category\") \\\n",
    "        .agg(count(\"*\").alias(\"main_cat_count\"))\n",
    "    main_cat_window = Window.partitionBy(\"order_id\").orderBy(col(\"main_cat_count\").desc())\n",
    "    most_common_main = main_cat_counts.withColumn(\n",
    "        \"rank\", row_number().over(main_cat_window)\n",
    "    ).filter(col(\"rank\") == 1).drop(\"rank\", \"main_cat_count\")\n",
    "\n",
    "    sub_cat_counts = df_items_with_cats.groupBy(\"order_id\", \"sub_category\") \\\n",
    "        .agg(count(\"*\").alias(\"sub_cat_count\"))\n",
    "    sub_cat_window = Window.partitionBy(\"order_id\").orderBy(col(\"sub_cat_count\").desc())\n",
    "    most_common_sub = sub_cat_counts.withColumn(\n",
    "        \"rank\", row_number().over(sub_cat_window)\n",
    "    ).filter(col(\"rank\") == 1).drop(\"rank\", \"sub_cat_count\")\n",
    "\n",
    "    order_categories = most_common_main.join(most_common_sub, on=\"order_id\", how=\"outer\")\n",
    "    final_df_with_cats = final_df.join(order_categories, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    return final_df_with_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "495cb5de-be1c-43ae-ac0b-e1ca0a5655b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/25]  Processing 2016_09 (silver_olist_orders_2016_09.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2016_09.parquet → 3 rows in 4.65s\n",
      "\n",
      "[2/25]  Processing 2016_10 (silver_olist_orders_2016_10.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2016_10.parquet → 308 rows in 3.55s\n",
      "\n",
      "[3/25]  Processing 2016_12 (silver_olist_orders_2016_12.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2016_12.parquet → 1 rows in 2.86s\n",
      "\n",
      "[4/25]  Processing 2017_01 (silver_olist_orders_2017_01.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_01.parquet → 789 rows in 3.22s\n",
      "\n",
      "[5/25]  Processing 2017_02 (silver_olist_orders_2017_02.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_02.parquet → 1733 rows in 3.11s\n",
      "\n",
      "[6/25]  Processing 2017_03 (silver_olist_orders_2017_03.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_03.parquet → 2641 rows in 3.16s\n",
      "\n",
      "[7/25]  Processing 2017_04 (silver_olist_orders_2017_04.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_04.parquet → 2391 rows in 3.06s\n",
      "\n",
      "[8/25]  Processing 2017_05 (silver_olist_orders_2017_05.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_05.parquet → 3660 rows in 3.14s\n",
      "\n",
      "[9/25]  Processing 2017_06 (silver_olist_orders_2017_06.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_06.parquet → 3217 rows in 3.17s\n",
      "\n",
      "[10/25]  Processing 2017_07 (silver_olist_orders_2017_07.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_07.parquet → 3969 rows in 3.26s\n",
      "\n",
      "[11/25]  Processing 2017_08 (silver_olist_orders_2017_08.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_08.parquet → 4293 rows in 3.12s\n",
      "\n",
      "[12/25]  Processing 2017_09 (silver_olist_orders_2017_09.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_09.parquet → 4243 rows in 3.12s\n",
      "\n",
      "[13/25]  Processing 2017_10 (silver_olist_orders_2017_10.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_10.parquet → 4568 rows in 3.1s\n",
      "\n",
      "[14/25]  Processing 2017_11 (silver_olist_orders_2017_11.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_11.parquet → 7451 rows in 3.12s\n",
      "\n",
      "[15/25]  Processing 2017_12 (silver_olist_orders_2017_12.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_12.parquet → 5624 rows in 3.07s\n",
      "\n",
      "[16/25]  Processing 2018_01 (silver_olist_orders_2018_01.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_01.parquet → 7220 rows in 3.09s\n",
      "\n",
      "[17/25]  Processing 2018_02 (silver_olist_orders_2018_02.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_02.parquet → 6694 rows in 3.13s\n",
      "\n",
      "[18/25]  Processing 2018_03 (silver_olist_orders_2018_03.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_03.parquet → 7188 rows in 3.04s\n",
      "\n",
      "[19/25]  Processing 2018_04 (silver_olist_orders_2018_04.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_04.parquet → 6934 rows in 3.12s\n",
      "\n",
      "[20/25]  Processing 2018_05 (silver_olist_orders_2018_05.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_05.parquet → 6853 rows in 3.15s\n",
      "\n",
      "[21/25]  Processing 2018_06 (silver_olist_orders_2018_06.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_06.parquet → 6160 rows in 3.13s\n",
      "\n",
      "[22/25]  Processing 2018_07 (silver_olist_orders_2018_07.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_07.parquet → 6273 rows in 3.11s\n",
      "\n",
      "[23/25]  Processing 2018_08 (silver_olist_orders_2018_08.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_08.parquet → 6452 rows in 3.19s\n",
      "\n",
      "[24/25]  Processing 2018_09 (silver_olist_orders_2018_09.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_09.parquet → 1 rows in 2.83s\n",
      "\n",
      "[25/25]  Processing 2018_10 (silver_olist_orders_2018_10.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_10.parquet → 0 rows in 1.8s\n",
      "\n",
      "===== Processing Summary =====\n",
      " Successfully processed: 25 files\n",
      "  - 2016_09: 3 rows in 4.65s\n",
      "  - 2016_10: 308 rows in 3.55s\n",
      "  - 2016_12: 1 rows in 2.86s\n",
      "  - 2017_01: 789 rows in 3.22s\n",
      "  - 2017_02: 1733 rows in 3.11s\n",
      "  - 2017_03: 2641 rows in 3.16s\n",
      "  - 2017_04: 2391 rows in 3.06s\n",
      "  - 2017_05: 3660 rows in 3.14s\n",
      "  - 2017_06: 3217 rows in 3.17s\n",
      "  - 2017_07: 3969 rows in 3.26s\n",
      "  - 2017_08: 4293 rows in 3.12s\n",
      "  - 2017_09: 4243 rows in 3.12s\n",
      "  - 2017_10: 4568 rows in 3.1s\n",
      "  - 2017_11: 7451 rows in 3.12s\n",
      "  - 2017_12: 5624 rows in 3.07s\n",
      "  - 2018_01: 7220 rows in 3.09s\n",
      "  - 2018_02: 6694 rows in 3.13s\n",
      "  - 2018_03: 7188 rows in 3.04s\n",
      "  - 2018_04: 6934 rows in 3.12s\n",
      "  - 2018_05: 6853 rows in 3.15s\n",
      "  - 2018_06: 6160 rows in 3.13s\n",
      "  - 2018_07: 6273 rows in 3.11s\n",
      "  - 2018_08: 6452 rows in 3.19s\n",
      "  - 2018_09: 1 rows in 2.83s\n",
      "  - 2018_10: 0 rows in 1.8s\n",
      "\n",
      " All files processed successfully\n"
     ]
    }
   ],
   "source": [
    "# Keep track of failures\n",
    "failed_files = []\n",
    "processed_files = []\n",
    "\n",
    "order_files = sorted(glob.glob(\"datamart/silver/orders/silver_olist_orders_*.parquet\"))\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"datamart/silver/order_logistics\", exist_ok=True)\n",
    "\n",
    "# Loop over files \n",
    "for idx, file_path in enumerate(order_files, 1):\n",
    "    \n",
    "    basename = os.path.basename(file_path)  \n",
    "    year_month = basename.replace(\"silver_olist_orders_\", \"\").replace(\".parquet\", \"\")\n",
    "\n",
    "    output_path = f\"datamart/silver/order_logistics/silver_olist_order_logistics_{year_month}.parquet\"\n",
    "    \n",
    "    print(f\"\\n[{idx}/{len(order_files)}]  Processing {year_month} ({basename})...\")\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\" Skipping {year_month} (already exists)\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Run feature engineering\n",
    "        final_df = build_order_features(spark, file_path)\n",
    "\n",
    "        # Save to parquet\n",
    "        final_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "        # Verify row count\n",
    "        count = final_df.count()\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "\n",
    "        print(f\"---> Saved: {output_path} → {count} rows in {duration}s\")\n",
    "\n",
    "        processed_files.append((year_month, count, duration))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed on {year_month}: {e}\")\n",
    "        failed_files.append((year_month, str(e)))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n===== Processing Summary =====\")\n",
    "print(f\" Successfully processed: {len(processed_files)} files\")\n",
    "for ym, count, duration in processed_files:\n",
    "    print(f\"  - {ym}: {count} rows in {duration}s\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\n Failed files: {len(failed_files)}\")\n",
    "    for ym, err in failed_files:\n",
    "        print(f\"  - {ym}: {err}\")\n",
    "else:\n",
    "    print(\"\\n All files processed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a75e5a67-d51a-4246-93af-b2de935082ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+---------+-----------+-------------------+--------------+----------------+--------------------+---------------+------------+\n",
      "|            order_id|order_purchase_timestamp|total_qty|total_price|total_freight_value|total_weight_g|total_volume_cm3|       total_density|  main_category|sub_category|\n",
      "+--------------------+------------------------+---------+-----------+-------------------+--------------+----------------+--------------------+---------------+------------+\n",
      "|8ab67a33f5086c597...|     2018-01-20 23:34:50|        1|       19.9|              17.63|         150.0|          2700.0| 0.05555555555555555|      telephony|          NA|\n",
      "|90c7f72cacf60b2a6...|     2018-01-04 18:18:46|        1|      129.0|              13.92|        6550.0|         31920.0| 0.20520050125313283|     stationery|          NA|\n",
      "|a322a4aa3b0a9b47a...|     2018-01-07 11:53:56|        1|       23.8|              21.15|         283.0|         11040.0|0.025634057971014493| sports_leisure|          NA|\n",
      "|aa60b99fd9b56eaff...|     2018-01-03 12:54:52|        1|      27.99|               15.1|         250.0|           816.0| 0.30637254901960786|      telephony|          NA|\n",
      "|ad09a8ebeb6151d83...|     2018-01-28 22:41:10|        1|     179.49|              16.02|         600.0|          6048.0|  0.0992063492063492|           auto|          NA|\n",
      "|b0c6259cdeba00d7f...|     2018-01-22 14:14:28|        1|       49.9|              16.79|         100.0|          8000.0|              0.0125|        perfume|          NA|\n",
      "|bb140c47651a535f2...|     2018-01-18 22:29:02|        2|       28.0|               23.7|         350.0|          5120.0|         0.068359375| bed_bath_table|          NA|\n",
      "|c3303e89dcbc08a6b...|     2018-01-24 16:52:57|        1|       65.0|              16.89|         600.0|          2816.0| 0.21306818181818182| sports_leisure|          NA|\n",
      "|8eebf60f909df0735...|     2018-01-03 21:21:00|        1|       79.0|              45.02|        8000.0|         57750.0| 0.13852813852813853|     stationery|          NA|\n",
      "|978dae5105c79039c...|     2018-01-09 22:45:13|        1|       79.0|               14.3|         200.0|          6000.0| 0.03333333333333333|fixed_telephony|          NA|\n",
      "+--------------------+------------------------+---------+-----------+-------------------+--------------+----------------+--------------------+---------------+------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df_orders_logistics = spark.read.parquet(\"datamart/silver/order_logistics/silver_olist_order_logistics_2018_01.parquet\")\n",
    "df_orders_logistics.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76752735-5f9b-42f2-8950-e1283efabee0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build Gold Table (Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cb3e0-1fe4-48f0-9a46-0f300154ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c818d8e7-a22a-40ab-abdf-ace25c516543",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inspect Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7d963-2611-4896-be4a-7fbbd641f4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5a6b17-8978-405a-8b04-6c6f4c7a3e52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build Gold Table (Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200137d-ca20-42b9-a3df-0b662024a899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70eeb8a3-7737-4556-a85c-e844b47f6454",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inspect Label Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20833e-9653-4c4c-9ce0-0023cca719b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974a0dd8-3602-42b8-b429-86cb7a2fe872",
   "metadata": {},
   "source": [
    "## Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97a33e6-9799-4a59-9588-40af6b036604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# End spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m.stop()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---completed job---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# End spark session\n",
    "spark.stop()\n",
    "\n",
    "print('\\n\\n---completed job---\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
