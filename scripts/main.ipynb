{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col, lower, trim, when,row_number, count,date_add, when,to_date\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "import utils.data_processing_bronze_table as bronze_processing\n",
    "import utils.data_processing_silver_table as silver_processing\n",
    "import utils.data_processing_gold_label_table as gold_label_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a822439-ea37-4662-a83e-1a4d48b1ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---starting job---\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/18 16:09:14 WARN Utils: Your hostname, Baohongs-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 10.169.0.196 instead (on interface en0)\n",
      "25/06/18 16:09:14 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/18 16:09:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n---starting job---\\n\\n')\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"olist_bronze_processing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cddd1-b87b-42dd-8158-8ecf9bd6b839",
   "metadata": {},
   "source": [
    "## Build Bronze Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e56e7-671b-4582-a4a7-9d551e83a226",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the bronze tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the bronze tables there.\n",
    "\n",
    "Need to have team meeting to resolve this\n",
    "\n",
    "I chose to run the main.py script, therefore subsequent code on Silver Tables built references the path from `app` folder to access the bronze tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99b6d22e-b746-4247-80a9-ecdd471b7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze root directory: datamart/bronze\n"
     ]
    }
   ],
   "source": [
    "# Create bronze root directory\n",
    "bronze_root = \"datamart/bronze\"\n",
    "os.makedirs(bronze_root, exist_ok=True)\n",
    "print(f\"Bronze root directory: {bronze_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20351f49-7a7b-4556-94af-db3b92c37668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Olist datasets...\n",
      "\n",
      "loaded data/olist_customers_dataset.csv  →  99,441 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bronze: datamart/bronze/customers/bronze_olist_customers.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_geolocation_dataset.csv  →  1,000,325 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bronze: datamart/bronze/geolocation/bronze_olist_geolocation.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_items_dataset.csv  →  112,650 rows\n",
      "saved bronze: datamart/bronze/order_items/bronze_olist_order_items.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_payments_dataset.csv  →  103,886 rows\n",
      "saved bronze: datamart/bronze/order_payments/bronze_olist_order_payments.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_reviews_dataset.csv  →  104,162 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bronze: datamart/bronze/order_reviews/bronze_olist_order_reviews.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_products_dataset.csv  →  32,951 rows\n",
      "saved bronze: datamart/bronze/products/bronze_olist_products.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_sellers_dataset.csv  →  3,095 rows\n",
      "saved bronze: datamart/bronze/sellers/bronze_olist_sellers.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/product_category_name_translation.csv  →  71 rows\n",
      "saved bronze: datamart/bronze/category_translation/bronze_product_category_translation.parquet\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process all Olist datasets\n",
    "print(\"\\nProcessing Olist datasets...\\n\")\n",
    "bronze_processing.process_olist_customers_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_geolocation_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_items_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_payments_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_reviews_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_products_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_sellers_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_product_cat_translation_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10db75df-e10c-477e-9433-c107cf8963bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-04: 6939 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_04.csv\n",
      "Month 2018-02: 6728 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_02.csv\n",
      "Month 2018-01: 7269 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_01.csv\n",
      "Month 2017-12: 5673 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_12.csv\n",
      "Month 2017-05: 3700 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_05.csv\n",
      "Month 2018-07: 6292 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_07.csv\n",
      "Month 2017-11: 7544 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_11.csv\n",
      "Month 2017-01: 800 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_01.csv\n",
      "Month 2016-09: 4 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_09.csv\n",
      "Month 2016-10: 324 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_10.csv\n",
      "Month 2017-03: 2682 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_03.csv\n",
      "Month 2017-04: 2404 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_04.csv\n",
      "Month 2017-10: 4631 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_10.csv\n",
      "Month 2017-09: 4285 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_09.csv\n",
      "Month 2017-02: 1780 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_02.csv\n",
      "Month 2018-09: 16 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_09.csv\n",
      "Month 2017-06: 3245 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_06.csv\n",
      "Month 2018-06: 6167 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_06.csv\n",
      "Month 2017-07: 4026 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_07.csv\n",
      "Month 2018-08: 6512 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_08.csv\n",
      "Month 2017-08: 4331 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_08.csv\n",
      "Month 2018-03: 7211 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_03.csv\n",
      "Month 2018-05: 6873 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_05.csv\n",
      "Month 2018-10: 4 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_10.csv\n",
      "Month 2016-12: 1 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_12.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: string, customer_id: string, order_status: string, order_purchase_timestamp: timestamp, order_approved_at: timestamp, order_delivered_carrier_date: timestamp, order_delivered_customer_date: timestamp, order_estimated_delivery_date: timestamp, snapshot_date: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process orders with monthly partitioning\n",
    "bronze_processing.process_olist_orders_bronze(bronze_root, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e42811-8a18-4f1d-bf62-7108dc267e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|f2a1d75b74d9ec748...|15ee900ec703c9a10...|                   68590|             jacunda|            PA|\n",
      "|f15272fe9d0e2ae32...|11e74a9cbe1158d1c...|                   15056|sao jose do rio p...|            SP|\n",
      "|7324ecb0ff143f561...|c6be127fa6e30c6f7...|                   13302|                 itu|            SP|\n",
      "|7accf3d920f47c07f...|a7f1a6dc9ba06844b...|                   45638|             coaraci|            BA|\n",
      "|3680a273ddb333253...|6cbfcc29787035834...|                   29700|            colatina|            ES|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "# I put the actual path due to the discrepancy in paths above. Will amend later\n",
    "# df_bronze = spark.read.parquet(\"../datamart/bronze/customers/bronze_olist_customers.parquet\")\n",
    "# df_bronze.show(5)\n",
    "\n",
    "# Can read\n",
    "\n",
    "\n",
    "# Maanoj for testing\n",
    "df_bronze = spark.read.parquet(\"datamart/bronze/customers/bronze_olist_customers.parquet\")\n",
    "df_bronze.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767bf08-b027-43bd-a695-43b5217fd756",
   "metadata": {},
   "source": [
    "## Build Silver Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d6071-2d84-491f-81d9-a2f0f692e526",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the silver tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the silver tables there.\n",
    "\n",
    "Need to have team meeting to resolve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dd2b29d3-9ac3-4246-a52f-d67fadf64940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver root directory: datamart/silver\n"
     ]
    }
   ],
   "source": [
    "# Create silver root directory\n",
    "silver_root = \"datamart/silver\"\n",
    "os.makedirs(silver_root, exist_ok=True)\n",
    "print(f\"Silver root directory: {silver_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1012f779-1308-4a32-9e1e-e6bfc9289cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all required output directories\n",
    "\n",
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)\n",
    "\n",
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)\n",
    "\n",
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)\n",
    "\n",
    "\n",
    "# Create silver directory to save products data\n",
    "silver_prod_directory = \"datamart/silver/products/\"\n",
    "if not os.path.exists(silver_prod_directory):\n",
    "    os.makedirs(silver_prod_directory)\n",
    "\n",
    "# # Create silver directory to save product_categories data\n",
    "# silver_prod_cat_directory = \"datamart/silver/product_categories/\"\n",
    "# if not os.path.exists(silver_prod_cat_directory):\n",
    "#     os.makedirs(silver_prod_cat_directory)\n",
    "\n",
    "# Create silver directory to save orders data\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "if not os.path.exists(silver_orders_directory):\n",
    "    os.makedirs(silver_orders_directory)\n",
    "\n",
    "# Create silver directory to save order_items data\n",
    "silver_order_items_directory = \"datamart/silver/order_items/\"\n",
    "if not os.path.exists(silver_order_items_directory):\n",
    "    os.makedirs(silver_order_items_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a4dee92-5661-46b9-85ad-e84d0448147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing bronze tables...\n",
      "loaded from: datamart/bronze/customers/bronze_olist_customers.parquet row count: 99441\n",
      "Number of duplicated 'customer_id': 0\n",
      "saved to: datamart/silver/customers/silver_olist_customers.parquet\n",
      "loaded from: datamart/bronze/sellers/bronze_olist_sellers.parquet row count: 3095\n",
      "Number of duplicated 'seller_id': 0\n",
      "saved to: datamart/silver/sellers/silver_olist_sellers.parquet\n",
      "loaded from: datamart/bronze/geolocation/bronze_olist_geolocation.parquet row count: 1000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/geolocation/silver_olist_geolocation.parquet\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'utils.data_processing_silver_table' has no attribute 'process_silver_olist_products'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m silver_processing\u001b[38;5;241m.\u001b[39mprocess_silver_olist_geolocation(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatamart/bronze/geolocation/\u001b[39m\u001b[38;5;124m\"\u001b[39m,silver_geo_directory, spark)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# add more below\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m silver_processing\u001b[38;5;241m.\u001b[39mprocess_silver_olist_products(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatamart/bronze/products/\u001b[39m\u001b[38;5;124m\"\u001b[39m,silver_prod_directory, spark)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# silver_processing.process_silver_olist_product_categories(\"../datamart/bronze/??/\",silver_prod_cat_directory, spark)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m silver_processing\u001b[38;5;241m.\u001b[39mprocess_silver_olist_orders(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatamart/bronze/orders/\u001b[39m\u001b[38;5;124m\"\u001b[39m,silver_orders_directory, spark)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'utils.data_processing_silver_table' has no attribute 'process_silver_olist_products'"
     ]
    }
   ],
   "source": [
    "# Process all bronze tables into silver\n",
    "print(\"\\nProcessing bronze tables...\")\n",
    "silver_processing.process_silver_olist_customers(\"datamart/bronze/customers/\",silver_cust_directory, spark)\n",
    "silver_processing.process_silver_olist_sellers(\"datamart/bronze/sellers/\",silver_sell_directory, spark)\n",
    "silver_processing.process_silver_olist_geolocation(\"datamart/bronze/geolocation/\",silver_geo_directory, spark)\n",
    "\n",
    "# add more below\n",
    "silver_processing.process_silver_olist_products(\"datamart/bronze/products/\",silver_prod_directory, spark)\n",
    "# silver_processing.process_silver_olist_product_categories(\"../datamart/bronze/??/\",silver_prod_cat_directory, spark)\n",
    "silver_processing.process_silver_olist_orders(\"datamart/bronze/orders/\",silver_orders_directory, spark)\n",
    "silver_processing.process_silver_olist_order_items(\"datamart/bronze/order_items/\",silver_order_items_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735fc5a-0b84-4d4d-9a39-5fcf13b359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process orders with monthly partitioning\n",
    "# add more below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9692dab1-3c00-427d-9abf-9319dadf04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "|                      49290|-11.274805005391439|-37.790795516967776|\n",
      "|                      49630|-10.605308055877686|-37.113027572631836|\n",
      "|                      55445|   -8.5616774559021|  -35.8295783996582|\n",
      "|                      57051| -9.655002400681779| -35.73440123893119|\n",
      "|                      57085| -9.558634171119103| -35.73914117079515|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df_silver = spark.read.parquet(\"datamart/silver/geolocation/silver_olist_geolocation.parquet\")\n",
    "df_silver.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b00aa2-e29a-4965-866f-a1f593ab4c23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Customer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fd1733-5963-4536-987b-528e045dba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c4513d-86f0-4807-9cc4-eb1d3a694448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_customers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_customers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"customer_id\": StringType(),\n",
    "        \"customer_unique_id\": StringType(),\n",
    "        \"customer_zip_code_prefix\": StringType(),\n",
    "        \"customer_city\": StringType(),\n",
    "        \"customer_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check customer_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"customer_id\").distinct().count()\n",
    "    duplicates_customer_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'customer_id': {duplicates_customer_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"customer_zip_code_prefix\",\n",
    "        F.lpad(col(\"customer_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_customers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ca153f-67d1-412a-a729-7080f9bc5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/customers/bronze_olist_customers.parquet row count: 99441\n",
      "Number of duplicated 'customer_id': 0\n",
      "saved to: datamart/silver/customers/silver_olist_customers.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_customers(\"datamart/bronze/customers/\",silver_cust_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f8aef-d715-44d9-9f81-c8c7202432b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f823b48-1e8c-4b5c-a38e-0921dbae1c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|99441|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"customer_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f0173-1008-4f1c-a776-f36daec1746b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Seller Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba12ae85-298f-454d-8a35-daddd3897a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5bea31b9-621e-4630-8b17-1106edf023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_sellers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_sellers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"seller_id\": StringType(),\n",
    "        \"seller_zip_code_prefix\": StringType(),\n",
    "        \"seller_city\": StringType(),\n",
    "        \"seller_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check seller_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"seller_id\").distinct().count()\n",
    "    duplicates_seller_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'seller_id': {duplicates_seller_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"seller_zip_code_prefix\",\n",
    "        F.lpad(col(\"seller_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_sellers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd3ef983-9e83-4970-85fd-d7358d2f3de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/sellers/bronze_olist_sellers.parquet row count: 3095\n",
      "Number of duplicated 'seller_id': 0\n",
      "saved to: datamart/silver/sellers/silver_olist_sellers.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_sellers(\"datamart/bronze/sellers/\",silver_sell_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1468622b-78f2-4fd7-a0b9-6c2e4b5a6cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066674d2-760b-47fe-89ef-0be33600f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5| 3095|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"seller_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73580bff-eb08-412b-b91e-59ed8767181a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Geolocation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8548b10-ed7f-41d6-b767-f23f4c771a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437f8bf-83cd-475d-a948-4bffd41e2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_geolocation(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_geolocation.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"geolocation_zip_code_prefix\": StringType(),\n",
    "        \"geolocation_lat\": FloatType(),\n",
    "        \"geolocation_lng\": FloatType(),\n",
    "        \"geolocation_city\": StringType(),\n",
    "        \"geolocation_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"geolocation_zip_code_prefix\",\n",
    "        F.lpad(col(\"geolocation_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "\n",
    "    # Deduplicate zipcodes by just taking the centroid (mean of lat,lng)\n",
    "    df_dedupe = df.groupBy(\"geolocation_zip_code_prefix\").agg(\n",
    "        F.avg(\"geolocation_lat\").alias(\"geolocation_lat\"),\n",
    "        F.avg(\"geolocation_lng\").alias(\"geolocation_lng\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_geolocation.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744285be-254a-482c-bbd4-f1b4dc73cef9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'process_silver_olist_geolocation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Run function manually to test\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m df = \u001b[43mprocess_silver_olist_geolocation\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m../datamart/bronze/geolocation/\u001b[39m\u001b[33m\"\u001b[39m,silver_geo_directory, spark)\n",
      "\u001b[31mNameError\u001b[39m: name 'process_silver_olist_geolocation' is not defined"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_geolocation(\"../datamart/bronze/geolocation/\",silver_geo_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a145539-6949-49b3-bc73-f95e92a0191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geolocation_zip_code_prefix: string (nullable = true)\n",
      " |-- geolocation_lat: double (nullable = true)\n",
      " |-- geolocation_lng: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a998b9-e0d1-46df-90f8-7ac1fc45901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|19177|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"geolocation_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5857ffa-e899-4d38-a856-92d3a9c2a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----+\n",
      "|geolocation_zip_code_prefix|count|\n",
      "+---------------------------+-----+\n",
      "+---------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check every geolocation_zip_code_prefix only has 1 count. Group by prefix and count occurrences\n",
    "df.groupBy(\"geolocation_zip_code_prefix\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .filter(\"count > 1\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc538d2b-2b37-478b-844a-7af3724f1dcb",
   "metadata": {},
   "source": [
    "### Build Products Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4e4909e9-3347-4b1f-9728-aba795fc18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save products data\n",
    "silver_prod_directory = \"datamart/silver/products/\"\n",
    "if not os.path.exists(silver_prod_directory):\n",
    "    os.makedirs(silver_prod_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a42f5caf-d676-4975-b67c-203483680798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_products(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_products.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # Rename columns due to spelling mistakes \n",
    "    df = df.withColumnRenamed(\"product_name_lenght\", \"product_name_length\") \\\n",
    "           .withColumnRenamed(\"product_description_lenght\", \"product_description_length\")\n",
    "\n",
    "    \n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"product_id\": StringType(),\n",
    "        \"product_category_name\": StringType(),\n",
    "        \"product_name_length\": DoubleType(),\n",
    "        \"product_description_length\": DoubleType(),\n",
    "        \"product_photos_qty\": DoubleType(),\n",
    "        \"product_weight_g\": DoubleType(),\n",
    "        \"product_length_cm\": DoubleType(),\n",
    "        \"product_height_cm\": DoubleType(),\n",
    "        \"product_width_cm\": DoubleType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Inputting missing values as NaN\n",
    "    df = df.fillna({\"product_category_name\": \"NaN\"})\n",
    "    df = df.fillna({\"product_name_length\": float('nan')}) \n",
    "    df = df.fillna({\"product_description_length\": float('nan')}) \n",
    "    df = df.fillna({\"product_photos_qty\": float('nan')}) \n",
    "    \n",
    "    # Check product_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"product_id\").distinct().count()\n",
    "    duplicates_product_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'product_id': {duplicates_product_id}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_products.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "02873004-1348-4cf8-9323-cf9a4692b4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/products/bronze_olist_products.parquet row count: 32951\n",
      "Number of duplicated 'product_id': 0\n",
      "saved to: datamart/silver/products/silver_olist_products.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_products(\"datamart/bronze/products/\",silver_prod_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b3864036-80cd-4e10-b1e8-9262992cd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_category_name: string (nullable = false)\n",
      " |-- product_name_length: double (nullable = false)\n",
      " |-- product_description_length: double (nullable = false)\n",
      " |-- product_photos_qty: double (nullable = false)\n",
      " |-- product_weight_g: double (nullable = true)\n",
      " |-- product_length_cm: double (nullable = true)\n",
      " |-- product_height_cm: double (nullable = true)\n",
      " |-- product_width_cm: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62d0c758-2904-4604-83eb-9fa01659f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|          product_id|product_category_name|product_name_length|product_description_length|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|1e9e8ef04dbcff454...|           perfumaria|               40.0|                     287.0|               1.0|           225.0|             16.0|             10.0|            14.0|\n",
      "|3aa071139cb16b67c...|                artes|               44.0|                     276.0|               1.0|          1000.0|             30.0|             18.0|            20.0|\n",
      "|96bd76ec8810374ed...|        esporte_lazer|               46.0|                     250.0|               1.0|           154.0|             18.0|              9.0|            15.0|\n",
      "|cef67bcfe19066a93...|                bebes|               27.0|                     261.0|               1.0|           371.0|             26.0|              4.0|            26.0|\n",
      "|9dc1a7de274444849...| utilidades_domest...|               37.0|                     402.0|               4.0|           625.0|             20.0|             17.0|            13.0|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df = spark.read.parquet(\"datamart/silver/products/silver_olist_products.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e4638-eb87-4613-9279-a740b36a712a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Product Categories Table??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761af31e-a1ce-48f7-b8c6-2458611a1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddad9d-b277-435c-bf4b-2cb863a93f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fcfbe42-badc-42ee-9b92-bf2b9a665a79",
   "metadata": {},
   "source": [
    "### Build Orders Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd19e8c-2ac8-4c03-9917-2c7073ef2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save orders data\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "if not os.path.exists(silver_orders_directory):\n",
    "    os.makedirs(silver_orders_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "821674c0-c4ed-415b-96a5-fef4be96d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_orders(bronze_directory, silver_directory, spark, partition_name):\n",
    "    filepath = os.path.join(bronze_directory, partition_name)\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "\n",
    "# def process_silver_olist_orders(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    \n",
    "    # partition_name = \"bronze_olist_orders.parquet\"\n",
    "    # filepath = bronze_directory + partition_name\n",
    "    # df = spark.read.parquet(filepath)\n",
    "    # print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"order_id\": StringType(),\n",
    "        \"customer_id\": StringType(),\n",
    "        \"order_status\": StringType(),\n",
    "        \"order_purchase_timestamp\": TimestampType(),\n",
    "        \"order_approved_at\": TimestampType(),\n",
    "        \"order_delivered_carrier_date\": TimestampType(),\n",
    "        \"order_delivered_customer_date\": TimestampType(),\n",
    "        \"order_estimated_delivery_date\": TimestampType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Removing Invalid order ids\n",
    "    # Load the Bronze table \n",
    "    df_order_items = spark.read.parquet(\"datamart/bronze/order_items/bronze_olist_order_items.parquet\") \n",
    "    \n",
    "    # Get distinct order IDs that exist in order items\n",
    "    valid_order_ids_df = df_order_items.select(\"order_id\").distinct()\n",
    "    \n",
    "    \n",
    "    # Keep only orders that exist in df_order_items\n",
    "    df_orders_clean = df.join(valid_order_ids_df, on=\"order_id\", how=\"inner\")\n",
    "    \n",
    "    # Count how many were dropped\n",
    "    dropped_orders = df.count() - df_orders_clean.count()\n",
    "    print(f\"Dropped {dropped_orders} orders with no items.\")\n",
    "\n",
    "    df = df_orders_clean\n",
    "\n",
    "\n",
    "    # Checking for invalid customer IDs\n",
    "    # Load df_customers from Bronze\n",
    "    df_customers = spark.read.parquet(\"datamart/bronze/customers/bronze_olist_customers.parquet\")  \n",
    "\n",
    "    # Get distinct valid customer IDs\n",
    "    valid_customer_ids_df = df_customers.select(\"customer_id\").distinct()\n",
    "    \n",
    "    # Perform a left anti join to find orders with invalid customer_id\n",
    "    invalid_orders = df.join(valid_customer_ids_df, on=\"customer_id\", how=\"left_anti\")\n",
    "    \n",
    "    # Count how many invalid customer IDs there are\n",
    "    invalid_customer_count = invalid_orders.count()\n",
    "\n",
    "    # Conditionally drop invalid orders\n",
    "    if invalid_customer_count > 0:\n",
    "        initial_count = df.count()\n",
    "        print(\"Dropping orders with invalid customer_id...\")\n",
    "        df = df.join(valid_customer_ids_df, on=\"customer_id\", how=\"inner\")\n",
    "        final_count = df.count()\n",
    "        dropped_count = initial_count - final_count\n",
    "        print(f\"Dropped {dropped_count} rows\")\n",
    "        \n",
    "    else:\n",
    "        print(\"All customer ids are valid — no need to drop!!\")\n",
    "\n",
    "\n",
    "\n",
    "    # Enforcing enum for order statuses\n",
    "    # Define valid statuses \n",
    "    valid_statuses = {\n",
    "        \"created\",\n",
    "        \"approved\",\n",
    "        \"processing\",\n",
    "        \"invoiced\",\n",
    "        \"shipped\",\n",
    "        \"delivered\",\n",
    "        \"canceled\",\n",
    "        \"unavailable\"\n",
    "    }\n",
    "    \n",
    "    # Clean and standardize the `order_status` column\n",
    "    df = df.withColumn(\"order_status\", trim(lower(col(\"order_status\"))))\n",
    "    \n",
    "    # dentify invalid statuses (those NOT in the valid_statuses set)\n",
    "    invalid_statuses_df = df.filter(~col(\"order_status\").isin(list(valid_statuses)))\n",
    "    \n",
    "    # Print the unique invalid statuses\n",
    "    invalid_statuses_list = invalid_statuses_df.select(\"order_status\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    if invalid_statuses_list:\n",
    "        print(f\"Invalid statuses found: {invalid_statuses_list}\")\n",
    "    else:\n",
    "        print(\"No invalid status found!!\")\n",
    "\n",
    "\n",
    "    \n",
    "    # # save silver table - IRL connect to database to write\n",
    "    # partition_name = \"silver_olist_orders_2016_09.parquet\"  \n",
    "    # filepath = silver_directory + partition_name\n",
    "    # df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    # print('saved to:', filepath)\n",
    "\n",
    "\n",
    "    # save \n",
    "    parquet_name = partition_name.replace(\"bronze\", \"silver\").replace(\".csv\", \".parquet\")\n",
    "    output_path = os.path.join(silver_directory, parquet_name)\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(\"-----> saved to:\", output_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c38f543-33b9-4ae0-ab4c-c34776d9ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Processing bronze_olist_orders_2016_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_09.csv row count: 4\n",
      "Dropped 1 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2016_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2016_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_10.csv row count: 324\n",
      "Dropped 16 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2016_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2016_12.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_12.csv row count: 1\n",
      "Dropped 0 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2016_12.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_01.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_01.csv row count: 800\n",
      "Dropped 11 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_01.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_02.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_02.csv row count: 1780\n",
      "Dropped 47 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_02.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_03.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_03.csv row count: 2682\n",
      "Dropped 41 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_03.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_04.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_04.csv row count: 2404\n",
      "Dropped 13 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_04.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_05.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_05.csv row count: 3700\n",
      "Dropped 40 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_05.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_06.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_06.csv row count: 3245\n",
      "Dropped 28 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_06.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_07.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_07.csv row count: 4026\n",
      "Dropped 57 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_07.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_08.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_08.csv row count: 4331\n",
      "Dropped 38 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_08.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_09.csv row count: 4285\n",
      "Dropped 42 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_10.csv row count: 4631\n",
      "Dropped 63 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_11.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_11.csv row count: 7544\n",
      "Dropped 93 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_11.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_12.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_12.csv row count: 5673\n",
      "Dropped 49 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2017_12.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_01.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_01.csv row count: 7269\n",
      "Dropped 49 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_01.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_02.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_02.csv row count: 6728\n",
      "Dropped 34 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_02.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_03.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_03.csv row count: 7211\n",
      "Dropped 23 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_03.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_04.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_04.csv row count: 6939\n",
      "Dropped 5 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_04.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_05.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_05.csv row count: 6873\n",
      "Dropped 20 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_05.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_06.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_06.csv row count: 6167\n",
      "Dropped 7 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_06.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_07.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_07.csv row count: 6292\n",
      "Dropped 19 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_07.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_08.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_08.csv row count: 6512\n",
      "Dropped 60 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_08.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_09.csv row count: 16\n",
      "Dropped 15 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_10.csv row count: 4\n",
      "Dropped 4 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/silver_olist_orders_2018_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "\n",
    "# Set base directory\n",
    "bronze_orders_directory = \"datamart/bronze/orders/\"\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "\n",
    "# List all CSV files in the bronze orders folder\n",
    "csv_files = [f for f in os.listdir(bronze_orders_directory) if f.endswith(\".csv\")]\n",
    "\n",
    "# Sort the files according to date\n",
    "csv_files.sort()\n",
    "\n",
    "# Loop through each file \n",
    "for partition_name in csv_files:\n",
    "    print(f\"\\n======== Processing {partition_name} ......... \\n\")\n",
    "    df = process_silver_olist_orders(bronze_orders_directory, silver_orders_directory, spark, partition_name)\n",
    "    \n",
    "    # Check schema enforced\n",
    "    df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57dda5c-b11c-4bcc-833c-89b9158b35f7",
   "metadata": {},
   "source": [
    "### Build Order_Items Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "950f9874-fa4c-47c1-823e-d9e6d21104e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save order_items data\n",
    "silver_order_items_directory = \"datamart/silver/order_items/\"\n",
    "if not os.path.exists(silver_order_items_directory):\n",
    "    os.makedirs(silver_order_items_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a05472f-0707-49c9-a85a-7a8addb30c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_order_items(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_order_items.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    \n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"order_id\": StringType(),\n",
    "        \"order_item_id\": LongType(),\n",
    "        \"product_id\": StringType(),\n",
    "        \"seller_id\": StringType(),\n",
    "        \"shipping_limit_date\": TimestampType(),\n",
    "        \"price\": DoubleType(),\n",
    "        \"freight_value\": DoubleType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    \n",
    "    # Checking for invalid seller IDs\n",
    "    # Load df_sellers from Bronze\n",
    "    df_sellers = spark.read.parquet(\"datamart/bronze/sellers/bronze_olist_sellers.parquet\")  \n",
    "\n",
    "    # Get distinct valid seller IDs\n",
    "    valid_seller_ids_df = df_sellers.select(\"seller_id\").distinct()\n",
    "    \n",
    "    # Perform a left anti join to find sellers with invalid seller_id\n",
    "    invalid_orders = df.join(valid_seller_ids_df, on=\"seller_id\", how=\"left_anti\")\n",
    "    \n",
    "    # Count how many invalid seller IDs there are\n",
    "    invalid_seller_count = invalid_orders.count()\n",
    "\n",
    "    # Conditionally drop invalid orders\n",
    "    if invalid_seller_count > 0:\n",
    "        initial_count = df.count()\n",
    "        print(\"Dropping orders with invalid seller_id...\")\n",
    "        df = df.join(valid_seller_ids_df, on=\"seller_id\", how=\"inner\")\n",
    "        final_count = df.count()\n",
    "        dropped_count = initial_count - final_count\n",
    "        print(f\"Dropped {dropped_count} rows\")\n",
    "        \n",
    "    else:\n",
    "        print(\"All seller ids are valid — no need to drop!!\")\n",
    "\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_order_items.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7b5663-92f0-426e-b6a8-9e7faa45d77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/order_items/bronze_olist_order_items.parquet row count: 112650\n",
      "All seller ids are valid — no need to drop!!\n",
      "saved to: datamart/silver/order_items/silver_olist_order_items.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_order_items(\"datamart/bronze/order_items/\",silver_order_items_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff299665-0d75-468e-acc7-15fd8f412f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a8e4aa-0994-4877-9981-a7927e3febbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|8ac26cb701a7887cc...|            1|4ebb87ba41ca44632...|7a67c85e85bb2ce85...|2017-05-22 16:05:14|109.99|        18.02|\n",
      "|8ac2728285fd4228f...|            1|8b90be4893a4277a9...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|         8.27|\n",
      "|8ac2728285fd4228f...|            2|fa94f25a73969e3a2...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|        16.55|\n",
      "|8ac2728285fd4228f...|            3|b01cedfa96d891427...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|259.99|        21.01|\n",
      "|8ac2728285fd4228f...|            4|fa94f25a73969e3a2...|004c9cd9d87a3c30c...|2017-03-15 14:09:17|109.99|        16.55|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df = spark.read.parquet(\"datamart/silver/order_items/silver_olist_order_items.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ff30d78-c8f6-4af9-bb89-57c326a8cb92",
   "metadata": {},
   "source": [
    "### Building Derived Table - Order logistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0d2287-aae6-410f-af54-4dd4f61863ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_order_features(spark, order_file_path):\n",
    "    # Read inputs\n",
    "    df_order_items = spark.read.parquet(\"datamart/silver/order_items/silver_olist_order_items.parquet\")\n",
    "    df_products = spark.read.parquet(\"datamart/silver/products/silver_olist_products.parquet\")\n",
    "    df_categories = spark.read.parquet(\"datamart/bronze/category_translation/bronze_product_category_translation.parquet\")\n",
    "    df_orders = spark.read.parquet(order_file_path)\n",
    "\n",
    "    order_metrics = df_order_items.groupBy(\"order_id\").agg(\n",
    "        F.max(\"order_item_id\").alias(\"total_qty\"),\n",
    "        F.sum(\"price\").alias(\"total_price\"),\n",
    "        F.sum(\"freight_value\").alias(\"total_freight_value\")\n",
    "    )\n",
    "\n",
    "    df_items_with_products = df_order_items.select(\"order_id\", \"product_id\") \\\n",
    "        .join(\n",
    "            df_products.select(\n",
    "                \"product_id\", \"product_weight_g\",\n",
    "                \"product_length_cm\", \"product_height_cm\", \"product_width_cm\"\n",
    "            ),\n",
    "            on=\"product_id\", how=\"left\"\n",
    "        )\n",
    "\n",
    "    df_items_with_products = df_items_with_products.withColumn(\n",
    "        \"product_volume_cm3\",\n",
    "        col(\"product_length_cm\") * col(\"product_height_cm\") * col(\"product_width_cm\")\n",
    "    )\n",
    "\n",
    "    product_metrics = df_items_with_products.groupBy(\"order_id\").agg(\n",
    "        F.sum(\"product_weight_g\").alias(\"total_weight_g\"),\n",
    "        F.sum(\"product_volume_cm3\").alias(\"total_volume_cm3\")\n",
    "    )\n",
    "\n",
    "    final_df = df_orders.select(\"order_id\", \"order_purchase_timestamp\") \\\n",
    "        .join(order_metrics, on=\"order_id\", how=\"inner\") \\\n",
    "        .join(product_metrics, on=\"order_id\", how=\"left\") \\\n",
    "        .withColumn(\n",
    "            \"total_density\",\n",
    "            when(col(\"total_volume_cm3\") != 0,\n",
    "                 col(\"total_weight_g\") / col(\"total_volume_cm3\")\n",
    "            ).otherwise(None)\n",
    "        )\n",
    "\n",
    "    df_items_with_cats = df_order_items.select(\"order_id\", \"product_id\") \\\n",
    "        .join(df_products.select(\"product_id\", \"product_category_name\"), on=\"product_id\", how=\"left\") \\\n",
    "        .join(df_categories.select(\"product_category_name\", \"main_category\", \"sub_category\"), on=\"product_category_name\", how=\"left\")\n",
    "\n",
    "    main_cat_counts = df_items_with_cats.groupBy(\"order_id\", \"main_category\") \\\n",
    "        .agg(count(\"*\").alias(\"main_cat_count\"))\n",
    "    main_cat_window = Window.partitionBy(\"order_id\").orderBy(col(\"main_cat_count\").desc())\n",
    "    most_common_main = main_cat_counts.withColumn(\n",
    "        \"rank\", row_number().over(main_cat_window)\n",
    "    ).filter(col(\"rank\") == 1).drop(\"rank\", \"main_cat_count\")\n",
    "\n",
    "    sub_cat_counts = df_items_with_cats.groupBy(\"order_id\", \"sub_category\") \\\n",
    "        .agg(count(\"*\").alias(\"sub_cat_count\"))\n",
    "    sub_cat_window = Window.partitionBy(\"order_id\").orderBy(col(\"sub_cat_count\").desc())\n",
    "    most_common_sub = sub_cat_counts.withColumn(\n",
    "        \"rank\", row_number().over(sub_cat_window)\n",
    "    ).filter(col(\"rank\") == 1).drop(\"rank\", \"sub_cat_count\")\n",
    "\n",
    "    order_categories = most_common_main.join(most_common_sub, on=\"order_id\", how=\"outer\")\n",
    "    final_df_with_cats = final_df.join(order_categories, on=\"order_id\", how=\"left\")\n",
    "\n",
    "    return final_df_with_cats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495cb5de-be1c-43ae-ac0b-e1ca0a5655b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[1/25]  Processing 2016_09 (silver_olist_orders_2016_09.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2016_09.parquet → 3 rows in 4.65s\n",
      "\n",
      "[2/25]  Processing 2016_10 (silver_olist_orders_2016_10.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2016_10.parquet → 308 rows in 3.55s\n",
      "\n",
      "[3/25]  Processing 2016_12 (silver_olist_orders_2016_12.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2016_12.parquet → 1 rows in 2.86s\n",
      "\n",
      "[4/25]  Processing 2017_01 (silver_olist_orders_2017_01.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_01.parquet → 789 rows in 3.22s\n",
      "\n",
      "[5/25]  Processing 2017_02 (silver_olist_orders_2017_02.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_02.parquet → 1733 rows in 3.11s\n",
      "\n",
      "[6/25]  Processing 2017_03 (silver_olist_orders_2017_03.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_03.parquet → 2641 rows in 3.16s\n",
      "\n",
      "[7/25]  Processing 2017_04 (silver_olist_orders_2017_04.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_04.parquet → 2391 rows in 3.06s\n",
      "\n",
      "[8/25]  Processing 2017_05 (silver_olist_orders_2017_05.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_05.parquet → 3660 rows in 3.14s\n",
      "\n",
      "[9/25]  Processing 2017_06 (silver_olist_orders_2017_06.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_06.parquet → 3217 rows in 3.17s\n",
      "\n",
      "[10/25]  Processing 2017_07 (silver_olist_orders_2017_07.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_07.parquet → 3969 rows in 3.26s\n",
      "\n",
      "[11/25]  Processing 2017_08 (silver_olist_orders_2017_08.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_08.parquet → 4293 rows in 3.12s\n",
      "\n",
      "[12/25]  Processing 2017_09 (silver_olist_orders_2017_09.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_09.parquet → 4243 rows in 3.12s\n",
      "\n",
      "[13/25]  Processing 2017_10 (silver_olist_orders_2017_10.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_10.parquet → 4568 rows in 3.1s\n",
      "\n",
      "[14/25]  Processing 2017_11 (silver_olist_orders_2017_11.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_11.parquet → 7451 rows in 3.12s\n",
      "\n",
      "[15/25]  Processing 2017_12 (silver_olist_orders_2017_12.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2017_12.parquet → 5624 rows in 3.07s\n",
      "\n",
      "[16/25]  Processing 2018_01 (silver_olist_orders_2018_01.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_01.parquet → 7220 rows in 3.09s\n",
      "\n",
      "[17/25]  Processing 2018_02 (silver_olist_orders_2018_02.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_02.parquet → 6694 rows in 3.13s\n",
      "\n",
      "[18/25]  Processing 2018_03 (silver_olist_orders_2018_03.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_03.parquet → 7188 rows in 3.04s\n",
      "\n",
      "[19/25]  Processing 2018_04 (silver_olist_orders_2018_04.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_04.parquet → 6934 rows in 3.12s\n",
      "\n",
      "[20/25]  Processing 2018_05 (silver_olist_orders_2018_05.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_05.parquet → 6853 rows in 3.15s\n",
      "\n",
      "[21/25]  Processing 2018_06 (silver_olist_orders_2018_06.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_06.parquet → 6160 rows in 3.13s\n",
      "\n",
      "[22/25]  Processing 2018_07 (silver_olist_orders_2018_07.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_07.parquet → 6273 rows in 3.11s\n",
      "\n",
      "[23/25]  Processing 2018_08 (silver_olist_orders_2018_08.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_08.parquet → 6452 rows in 3.19s\n",
      "\n",
      "[24/25]  Processing 2018_09 (silver_olist_orders_2018_09.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_09.parquet → 1 rows in 2.83s\n",
      "\n",
      "[25/25]  Processing 2018_10 (silver_olist_orders_2018_10.parquet)...\n",
      "---> Saved: datamart/silver/order_logistics/silver_olist_order_logistics_2018_10.parquet → 0 rows in 1.8s\n",
      "\n",
      "===== Processing Summary =====\n",
      " Successfully processed: 25 files\n",
      "  - 2016_09: 3 rows in 4.65s\n",
      "  - 2016_10: 308 rows in 3.55s\n",
      "  - 2016_12: 1 rows in 2.86s\n",
      "  - 2017_01: 789 rows in 3.22s\n",
      "  - 2017_02: 1733 rows in 3.11s\n",
      "  - 2017_03: 2641 rows in 3.16s\n",
      "  - 2017_04: 2391 rows in 3.06s\n",
      "  - 2017_05: 3660 rows in 3.14s\n",
      "  - 2017_06: 3217 rows in 3.17s\n",
      "  - 2017_07: 3969 rows in 3.26s\n",
      "  - 2017_08: 4293 rows in 3.12s\n",
      "  - 2017_09: 4243 rows in 3.12s\n",
      "  - 2017_10: 4568 rows in 3.1s\n",
      "  - 2017_11: 7451 rows in 3.12s\n",
      "  - 2017_12: 5624 rows in 3.07s\n",
      "  - 2018_01: 7220 rows in 3.09s\n",
      "  - 2018_02: 6694 rows in 3.13s\n",
      "  - 2018_03: 7188 rows in 3.04s\n",
      "  - 2018_04: 6934 rows in 3.12s\n",
      "  - 2018_05: 6853 rows in 3.15s\n",
      "  - 2018_06: 6160 rows in 3.13s\n",
      "  - 2018_07: 6273 rows in 3.11s\n",
      "  - 2018_08: 6452 rows in 3.19s\n",
      "  - 2018_09: 1 rows in 2.83s\n",
      "  - 2018_10: 0 rows in 1.8s\n",
      "\n",
      " All files processed successfully\n"
     ]
    }
   ],
   "source": [
    "# Keep track of failures\n",
    "failed_files = []\n",
    "processed_files = []\n",
    "\n",
    "order_files = sorted(glob.glob(\"datamart/silver/orders/silver_olist_orders_*.parquet\"))\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"datamart/silver/order_logistics\", exist_ok=True)\n",
    "\n",
    "# Loop over files \n",
    "for idx, file_path in enumerate(order_files, 1):\n",
    "    \n",
    "    basename = os.path.basename(file_path)  \n",
    "    year_month = basename.replace(\"silver_olist_orders_\", \"\").replace(\".parquet\", \"\")\n",
    "\n",
    "    output_path = f\"datamart/silver/order_logistics/silver_olist_order_logistics_{year_month}.parquet\"\n",
    "    \n",
    "    print(f\"\\n[{idx}/{len(order_files)}]  Processing {year_month} ({basename})...\")\n",
    "\n",
    "    # Skip if already exists\n",
    "    if os.path.exists(output_path):\n",
    "        print(f\" Skipping {year_month} (already exists)\")\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Run feature engineering\n",
    "        final_df = build_order_features(spark, file_path)\n",
    "\n",
    "        # Save to parquet\n",
    "        final_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "        # Verify row count\n",
    "        count = final_df.count()\n",
    "        duration = round(time.time() - start_time, 2)\n",
    "\n",
    "        print(f\"---> Saved: {output_path} → {count} rows in {duration}s\")\n",
    "\n",
    "        processed_files.append((year_month, count, duration))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Failed on {year_month}: {e}\")\n",
    "        failed_files.append((year_month, str(e)))\n",
    "\n",
    "# Summary\n",
    "print(\"\\n===== Processing Summary =====\")\n",
    "print(f\" Successfully processed: {len(processed_files)} files\")\n",
    "for ym, count, duration in processed_files:\n",
    "    print(f\"  - {ym}: {count} rows in {duration}s\")\n",
    "\n",
    "if failed_files:\n",
    "    print(f\"\\n Failed files: {len(failed_files)}\")\n",
    "    for ym, err in failed_files:\n",
    "        print(f\"  - {ym}: {err}\")\n",
    "else:\n",
    "    print(\"\\n All files processed successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75e5a67-d51a-4246-93af-b2de935082ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------------+---------+-----------+-------------------+--------------+----------------+--------------------+---------------+------------+\n",
      "|            order_id|order_purchase_timestamp|total_qty|total_price|total_freight_value|total_weight_g|total_volume_cm3|       total_density|  main_category|sub_category|\n",
      "+--------------------+------------------------+---------+-----------+-------------------+--------------+----------------+--------------------+---------------+------------+\n",
      "|8ab67a33f5086c597...|     2018-01-20 23:34:50|        1|       19.9|              17.63|         150.0|          2700.0| 0.05555555555555555|      telephony|          NA|\n",
      "|90c7f72cacf60b2a6...|     2018-01-04 18:18:46|        1|      129.0|              13.92|        6550.0|         31920.0| 0.20520050125313283|     stationery|          NA|\n",
      "|a322a4aa3b0a9b47a...|     2018-01-07 11:53:56|        1|       23.8|              21.15|         283.0|         11040.0|0.025634057971014493| sports_leisure|          NA|\n",
      "|aa60b99fd9b56eaff...|     2018-01-03 12:54:52|        1|      27.99|               15.1|         250.0|           816.0| 0.30637254901960786|      telephony|          NA|\n",
      "|ad09a8ebeb6151d83...|     2018-01-28 22:41:10|        1|     179.49|              16.02|         600.0|          6048.0|  0.0992063492063492|           auto|          NA|\n",
      "|b0c6259cdeba00d7f...|     2018-01-22 14:14:28|        1|       49.9|              16.79|         100.0|          8000.0|              0.0125|        perfume|          NA|\n",
      "|bb140c47651a535f2...|     2018-01-18 22:29:02|        2|       28.0|               23.7|         350.0|          5120.0|         0.068359375| bed_bath_table|          NA|\n",
      "|c3303e89dcbc08a6b...|     2018-01-24 16:52:57|        1|       65.0|              16.89|         600.0|          2816.0| 0.21306818181818182| sports_leisure|          NA|\n",
      "|8eebf60f909df0735...|     2018-01-03 21:21:00|        1|       79.0|              45.02|        8000.0|         57750.0| 0.13852813852813853|     stationery|          NA|\n",
      "|978dae5105c79039c...|     2018-01-09 22:45:13|        1|       79.0|               14.3|         200.0|          6000.0| 0.03333333333333333|fixed_telephony|          NA|\n",
      "+--------------------+------------------------+---------+-----------+-------------------+--------------+----------------+--------------------+---------------+------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df_orders_logistics = spark.read.parquet(\"datamart/silver/order_logistics/silver_olist_order_logistics_2018_01.parquet\")\n",
    "df_orders_logistics.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76752735-5f9b-42f2-8950-e1283efabee0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build Gold Table (Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cb3e0-1fe4-48f0-9a46-0f300154ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c818d8e7-a22a-40ab-abdf-ace25c516543",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inspect Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7d963-2611-4896-be4a-7fbbd641f4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5a6b17-8978-405a-8b04-6c6f4c7a3e52",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Build Gold Table (Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "38cdcd2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gold datalake\n",
    "silver_directory = \"datamart/silver\"\n",
    "gold_directory = \"datamart/gold\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a988a149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building label store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving labels: 100%|██████████| 1/1 [00:00<00:00,  6.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label store Completed\n",
      "Number of rows in label store: 96478\n"
     ]
    }
   ],
   "source": [
    "partitions_list = ['2017-10-04']\n",
    "y= gold_label_processing.process_gold_label(silver_directory, gold_directory, partitions_list, spark)\n",
    "orders = y.toPandas()\n",
    "\n",
    "print(f\"Number of rows in label store: {orders.shape[0]}\")\n",
    "#orders.groupby('snapshot_date').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "09bdb07e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>miss_delivery_sla</th>\n",
       "      <th>snapshot_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>95bfa2a85ef50d3192609d8f29b92cf9</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>c3fd670b03599718895218d479f660b6</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b6c70f4b37438a78c820423809997c20</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ab53b19e9f59776c6556ebf49e85a52c</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9913ce9487d390ef37cd3b6cc3883f0e</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>d36b13fdc087b62c490a9db5c0e0a913</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>d4304f4104fca54e2a93b03e5b04962b</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>d9e98b1f6961932f22bf340d0153bbad</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>f495e955026183e7f6bbb3dac79b88e6</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>f7059c86772ca15e9640e43be7c31fb1</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             order_id  miss_delivery_sla snapshot_date\n",
       "0    95bfa2a85ef50d3192609d8f29b92cf9                  0    2017-10-04\n",
       "1    c3fd670b03599718895218d479f660b6                  1    2017-10-04\n",
       "2    b6c70f4b37438a78c820423809997c20                  1    2017-10-04\n",
       "3    ab53b19e9f59776c6556ebf49e85a52c                  0    2017-10-04\n",
       "4    9913ce9487d390ef37cd3b6cc3883f0e                  1    2017-10-04\n",
       "..                                ...                ...           ...\n",
       "151  d36b13fdc087b62c490a9db5c0e0a913                  0    2017-10-04\n",
       "152  d4304f4104fca54e2a93b03e5b04962b                  0    2017-10-04\n",
       "153  d9e98b1f6961932f22bf340d0153bbad                  0    2017-10-04\n",
       "154  f495e955026183e7f6bbb3dac79b88e6                  0    2017-10-04\n",
       "155  f7059c86772ca15e9640e43be7c31fb1                  0    2017-10-04\n",
       "\n",
       "[156 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_silver_table(table, silver_directory, spark):\n",
    "    \"\"\"\n",
    "    Helper function to read all partitions of a silver table\n",
    "    \"\"\"\n",
    "    folder_path = os.path.join(silver_directory, table)\n",
    "    files_list = [os.path.join(folder_path, os.path.basename(f)) for f in glob.glob(os.path.join(folder_path, '*'))]\n",
    "    df = spark.read.option(\"header\", \"true\").parquet(*files_list)\n",
    "    return df\n",
    "\n",
    "gold_directory = \"datamart/gold\"\n",
    "order_df = read_silver_table('label_store', gold_directory, spark)\n",
    "order_df = order_df.toPandas()\n",
    "order_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27ea7ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timestamp('2016-09-04 21:15:19')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "order_df['order_purchase_timestamp'].min()\n",
    "#max_date = '2018-09-03'\n",
    "#min_date = '2016-09-04'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c83a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_silver_table(table, silver_directory, spark):\n",
    "    \"\"\"\n",
    "    Helper function to read all partitions of a silver table\n",
    "    \"\"\"\n",
    "    folder_path = os.path.join(silver_directory, table)\n",
    "    files_list = [os.path.join(folder_path, os.path.basename(f)) for f in glob.glob(os.path.join(folder_path, '*'))]\n",
    "    df = spark.read.option(\"header\", \"true\").parquet(*files_list)\n",
    "    return df\n",
    "\n",
    "############################\n",
    "# Label Store\n",
    "############################\n",
    "def build_label_store(sla, df):\n",
    "    \"\"\"\n",
    "    Function to build label store\n",
    "    \"\"\"\n",
    "    ####################\n",
    "    # Create labels\n",
    "    ####################\n",
    "\n",
    "    # get customer at mob\n",
    "    df = df.filter(col(\"order_status\") == 'delivered')\n",
    "\n",
    "    # get label\n",
    "    df = df.withColumn(\"order_purchase_timestamp\", to_date(col(\"order_purchase_timestamp\")))\n",
    "    df = df.withColumn(\"snapshot_date\", col(\"order_purchase_timestamp\"))\n",
    "    df = df.withColumn(\"miss_delivery_sla\", when(col(\"order_delivered_customer_date\") > date_add(col(\"snapshot_date\"), sla), 1).otherwise(0))\n",
    "\n",
    "    # select columns to save\n",
    "    df = df.select(\"order_id\", \"miss_delivery_sla\", \"snapshot_date\")\n",
    "\n",
    "    return df\n",
    "\n",
    "############################\n",
    "# Pipeline\n",
    "############################\n",
    "\n",
    "def process_gold_label(silver_directory, gold_directory, partitions_list, spark):\n",
    "    \"\"\"\n",
    "    Wrapper function to build all gold tables\n",
    "    \"\"\"\n",
    "    # Read silver tables\n",
    "    orders_df = read_silver_table('orders', silver_directory, spark)\n",
    "\n",
    "    # Build label store\n",
    "    print(\"Building label store...\")\n",
    "    df_label = build_label_store(14, orders_df)\n",
    "\n",
    "    for date_str in tqdm(partitions_list, total=len(partitions_list), desc=\"Saving labels\"):\n",
    "        partition_name = date_str.replace('-','_') + '.parquet'\n",
    "        label_filepath = os.path.join(gold_directory, 'label_store', partition_name)\n",
    "        df_label.filter(col('snapshot_date') == date_str).write.mode('overwrite').parquet(label_filepath)\n",
    "        #df_label_filtered = df_label.filter(col('snapshot_date') == date_str)\n",
    "\n",
    "    print(\"Label store Completed\")\n",
    "\n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200137d-ca20-42b9-a3df-0b662024a899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_gold_label(silver_directory, gold_directory, partitions_list, spark):\n",
    "    \"\"\"\n",
    "    Wrapper function to build all gold tables\n",
    "    \"\"\"\n",
    "    # Read silver tables\n",
    "    orders_df = read_silver_table('orders', silver_directory, spark)\n",
    "\n",
    "    # Build label store\n",
    "    print(\"Building label store...\")\n",
    "    df_label = build_label_store(14, orders_df)\n",
    "\n",
    "    for date_str in tqdm(partitions_list, total=len(partitions_list), desc=\"Saving labels\"):\n",
    "        partition_name = date_str.replace('-','_') + '.parquet'\n",
    "        label_filepath = os.path.join(gold_directory, 'label_store', partition_name)\n",
    "        df_label.filter(col('snapshot_date') == date_str).write.mode('overwrite').parquet(label_filepath)\n",
    "        #df_label_filtered = df_label.filter(col('snapshot_date') == date_str)\n",
    "\n",
    "    print(\"Label store Completed\")\n",
    "\n",
    "    return df_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94120157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building gold feature tables...\n",
      "Building label store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving labels: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label store Completed\n",
      "Number of rows in label store: 96478\n",
      "Gold feature tables built successfully from start date: ['2017-01-01']\n"
     ]
    }
   ],
   "source": [
    "start_date_str = ['2017-01-01']\n",
    "\n",
    "print(\"Building gold feature tables...\")\n",
    "# Create gold datalake\n",
    "silver_directory = \"datamart/silver\"\n",
    "gold_directory = \"datamart/gold\"\n",
    "\n",
    "if not os.path.exists(gold_directory):\n",
    "    os.makedirs(gold_directory)\n",
    "\n",
    "# Build gold tables\n",
    "y = process_gold_label(silver_directory, gold_directory, start_date_str, spark)\n",
    "\n",
    "# Check for the rows ingested\n",
    "y_pdf = y.toPandas()\n",
    "y_count = y_pdf.shape[0]\n",
    "print(f\"Number of rows in label store: {y_pdf.shape[0]}\")\n",
    "\n",
    "print(f\"Gold feature tables built successfully from start date: {start_date_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70eeb8a3-7737-4556-a85c-e844b47f6454",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Inspect Label Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20833e-9653-4c4c-9ce0-0023cca719b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974a0dd8-3602-42b8-b429-86cb7a2fe872",
   "metadata": {},
   "source": [
    "## Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b97a33e6-9799-4a59-9588-40af6b036604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# End spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m.stop()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---completed job---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# End spark session\n",
    "spark.stop()\n",
    "\n",
    "print('\\n\\n---completed job---\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
