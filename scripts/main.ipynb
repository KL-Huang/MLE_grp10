{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b50484d2-bfa4-40b6-82cd-e8bf86187cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import pprint\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.sql.types import StringType, IntegerType, FloatType, DateType, DoubleType, TimestampType, LongType\n",
    "from pyspark.sql.functions import col, lower, trim\n",
    "\n",
    "import utils.data_processing_bronze_table as bronze_processing\n",
    "import utils.data_processing_silver_table as silver_processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c91bb1-bcf0-4195-90f3-dc88806ebf8c",
   "metadata": {},
   "source": [
    "## set up pyspark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a822439-ea37-4662-a83e-1a4d48b1ab8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---starting job---\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/17 16:34:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "print('\\n\\n---starting job---\\n\\n')\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "    .appName(\"olist_bronze_processing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Set log level to ERROR to hide warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5cddd1-b87b-42dd-8158-8ecf9bd6b839",
   "metadata": {},
   "source": [
    "## Build Bronze Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17e56e7-671b-4582-a4a7-9d551e83a226",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the bronze tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the bronze tables there.\n",
    "\n",
    "Need to have team meeting to resolve this\n",
    "\n",
    "I chose to run the main.py script, therefore subsequent code on Silver Tables built references the path from `app` folder to access the bronze tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b6d22e-b746-4247-80a9-ecdd471b7dc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bronze root directory: datamart/bronze\n"
     ]
    }
   ],
   "source": [
    "# Create bronze root directory\n",
    "bronze_root = \"datamart/bronze\"\n",
    "os.makedirs(bronze_root, exist_ok=True)\n",
    "print(f\"Bronze root directory: {bronze_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20351f49-7a7b-4556-94af-db3b92c37668",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Olist datasets...\n",
      "\n",
      "loaded data/olist_customers_dataset.csv  →  99,441 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bronze: datamart/bronze/customers/bronze_olist_customers.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_geolocation_dataset.csv  →  1,000,325 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved bronze: datamart/bronze/geolocation/bronze_olist_geolocation.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_items_dataset.csv  →  112,650 rows\n",
      "saved bronze: datamart/bronze/order_items/bronze_olist_order_items.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_payments_dataset.csv  →  103,886 rows\n",
      "saved bronze: datamart/bronze/order_payments/bronze_olist_order_payments.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_order_reviews_dataset.csv  →  104,162 rows\n",
      "saved bronze: datamart/bronze/order_reviews/bronze_olist_order_reviews.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_products_dataset.csv  →  32,951 rows\n",
      "saved bronze: datamart/bronze/products/bronze_olist_products.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/olist_sellers_dataset.csv  →  3,095 rows\n",
      "saved bronze: datamart/bronze/sellers/bronze_olist_sellers.parquet\n",
      "-------------------------------------------------\n",
      "loaded data/product_category_name_translation.csv  →  71 rows\n",
      "saved bronze: datamart/bronze/category_translation/bronze_product_category_translation.parquet\n",
      "-------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Process all Olist datasets\n",
    "print(\"\\nProcessing Olist datasets...\\n\")\n",
    "bronze_processing.process_olist_customers_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_geolocation_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_items_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_payments_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_order_reviews_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_products_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_olist_sellers_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')\n",
    "bronze_processing.process_product_cat_translation_bronze(bronze_root, spark)\n",
    "print('-------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10db75df-e10c-477e-9433-c107cf8963bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-04: 6939 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-02: 6728 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-01: 7269 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-12: 5673 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-05: 3700 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_05.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-07: 6292 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-11: 7544 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_11.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-01: 800 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_01.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2016-09: 4 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2016-10: 324 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-03: 2682 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-04: 2404 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_04.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-10: 4631 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-09: 4285 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-02: 1780 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_02.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-09: 16 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_09.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-06: 3245 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-06: 6167 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_06.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-07: 4026 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_07.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-08: 6512 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_08.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2017-08: 4331 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2017_08.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-03: 7211 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_03.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-05: 6873 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_05.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2018-10: 4 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2018_10.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month 2016-12: 1 rows\n",
      "Saved to: datamart/bronze/orders/bronze_olist_orders_2016_12.csv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[order_id: string, customer_id: string, order_status: string, order_purchase_timestamp: timestamp, order_approved_at: timestamp, order_delivered_carrier_date: timestamp, order_delivered_customer_date: timestamp, order_estimated_delivery_date: timestamp, snapshot_date: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process orders with monthly partitioning\n",
    "bronze_processing.process_olist_orders_bronze(bronze_root, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76e42811-8a18-4f1d-bf62-7108dc267e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|503840d4f2a1a7609...|ffc4233210eac4ec1...|                   14811|          araraquara|            SP|\n",
      "|52e73a5d0a1d4c56b...|b43530186123fb6d9...|                   62625|               missi|            CE|\n",
      "|16cb62869f9719571...|c3cc321141423ab8a...|                   55560|           barreiros|            PE|\n",
      "|4979ba0e6037e4b28...|80768413a59684f1e...|                   29307|cachoeiro de itap...|            ES|\n",
      "|11ec4bc0610184925...|bd836cf4fce7f808b...|                   22420|      rio de janeiro|            RJ|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "# I put the actual path due to the discrepancy in paths above. Will amend later\n",
    "# df_bronze = spark.read.parquet(\"../datamart/bronze/customers/bronze_olist_customers.parquet\")\n",
    "# df_bronze.show(5)\n",
    "\n",
    "# Can read\n",
    "\n",
    "\n",
    "# Maanoj for testing\n",
    "df_bronze = spark.read.parquet(\"datamart/bronze/customers/bronze_olist_customers.parquet\")\n",
    "df_bronze.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d767bf08-b027-43bd-a695-43b5217fd756",
   "metadata": {},
   "source": [
    "## Build Silver Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573d6071-2d84-491f-81d9-a2f0f692e526",
   "metadata": {},
   "source": [
    "Important note: There is some discrepancy in where the datamart folder is created when the main.py script is run vs this Jupyter notebook is run.\n",
    "\n",
    "* This Jupyter notebook will create the datamart folder inside `scripts` folder and output the silver tables there.\n",
    "* When you run the main.py script, the datamart folder will be created inside `app` folder (i.e. root) and output the silver tables there.\n",
    "\n",
    "Need to have team meeting to resolve this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd2b29d3-9ac3-4246-a52f-d67fadf64940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silver root directory: datamart/silver\n"
     ]
    }
   ],
   "source": [
    "# Create silver root directory\n",
    "silver_root = \"datamart/silver\"\n",
    "os.makedirs(silver_root, exist_ok=True)\n",
    "print(f\"Silver root directory: {silver_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1012f779-1308-4a32-9e1e-e6bfc9289cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all required output directories\n",
    "\n",
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)\n",
    "\n",
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)\n",
    "\n",
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)\n",
    "\n",
    "\n",
    "# Create silver directory to save products data\n",
    "silver_prod_directory = \"datamart/silver/products/\"\n",
    "if not os.path.exists(silver_prod_directory):\n",
    "    os.makedirs(silver_prod_directory)\n",
    "\n",
    "# # Create silver directory to save product_categories data\n",
    "# silver_prod_cat_directory = \"datamart/silver/product_categories/\"\n",
    "# if not os.path.exists(silver_prod_cat_directory):\n",
    "#     os.makedirs(silver_prod_cat_directory)\n",
    "\n",
    "# Create silver directory to save orders data\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "if not os.path.exists(silver_orders_directory):\n",
    "    os.makedirs(silver_orders_directory)\n",
    "\n",
    "# Create silver directory to save order_items data\n",
    "silver_order_items_directory = \"datamart/silver/order_items/\"\n",
    "if not os.path.exists(silver_order_items_directory):\n",
    "    os.makedirs(silver_order_items_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a4dee92-5661-46b9-85ad-e84d0448147f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing bronze tables...\n",
      "loaded from: ../datamart/bronze/customers/bronze_olist_customers.parquet row count: 99441\n",
      "Number of duplicated 'customer_id': 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/customers/silver_olist_customers.parquet\n",
      "loaded from: ../datamart/bronze/sellers/bronze_olist_sellers.parquet row count: 3095\n",
      "Number of duplicated 'seller_id': 0\n",
      "saved to: datamart/silver/sellers/silver_olist_sellers.parquet\n",
      "loaded from: ../datamart/bronze/geolocation/bronze_olist_geolocation.parquet row count: 1000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/geolocation/silver_olist_geolocation.parquet\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[geolocation_zip_code_prefix: string, geolocation_lat: double, geolocation_lng: double]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Process all bronze tables into silver\n",
    "print(\"\\nProcessing bronze tables...\")\n",
    "silver_processing.process_silver_olist_customers(\"../datamart/bronze/customers/\",silver_cust_directory, spark)\n",
    "silver_processing.process_silver_olist_sellers(\"../datamart/bronze/sellers/\",silver_sell_directory, spark)\n",
    "silver_processing.process_silver_olist_geolocation(\"../datamart/bronze/geolocation/\",silver_geo_directory, spark)\n",
    "# add more below\n",
    "silver_processing.process_silver_olist_products(\"../datamart/bronze/products/\",silver_prod_directory, spark)\n",
    "# silver_processing.process_silver_olist_product_categories(\"../datamart/bronze/??/\",silver_prod_cat_directory, spark)\n",
    "silver_processing.process_silver_olist_orders(\"../datamart/bronze/orders/\",silver_orders_directory, spark)\n",
    "silver_processing.process_silver_olist_order_items(\"../datamart/bronze/order_items/\",silver_order_items_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7735fc5a-0b84-4d4d-9a39-5fcf13b359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process orders with monthly partitioning\n",
    "# add more below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9692dab1-3c00-427d-9abf-9319dadf04d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+-------------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "|                      49290|-11.274805005391439|-37.790795516967776|\n",
      "|                      49630|-10.605308055877686|-37.113027572631836|\n",
      "|                      55445|   -8.5616774559021|  -35.8295783996582|\n",
      "|                      57051| -9.655002400681779| -35.73440123893119|\n",
      "|                      57085| -9.558634171119103| -35.73914117079515|\n",
      "+---------------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df_silver = spark.read.parquet(\"datamart/silver/geolocation/silver_olist_geolocation.parquet\")\n",
    "df_silver.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b00aa2-e29a-4965-866f-a1f593ab4c23",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Customer Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81fd1733-5963-4536-987b-528e045dba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save customer data\n",
    "silver_cust_directory = \"datamart/silver/customers/\"\n",
    "if not os.path.exists(silver_cust_directory):\n",
    "    os.makedirs(silver_cust_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f8c4513d-86f0-4807-9cc4-eb1d3a694448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_customers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_customers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"customer_id\": StringType(),\n",
    "        \"customer_unique_id\": StringType(),\n",
    "        \"customer_zip_code_prefix\": StringType(),\n",
    "        \"customer_city\": StringType(),\n",
    "        \"customer_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check customer_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"customer_id\").distinct().count()\n",
    "    duplicates_customer_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'customer_id': {duplicates_customer_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"customer_zip_code_prefix\",\n",
    "        F.lpad(col(\"customer_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_customers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14ca153f-67d1-412a-a729-7080f9bc5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: ../datamart/bronze/customers/bronze_olist_customers.parquet row count: 99441\n",
      "Number of duplicated 'customer_id': 0\n",
      "saved to: datamart/silver/customers/silver_olist_customers.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_customers(\"../datamart/bronze/customers/\",silver_cust_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "825f8aef-d715-44d9-9f81-c8c7202432b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: string (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f823b48-1e8c-4b5c-a38e-0921dbae1c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|99441|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"customer_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2f0173-1008-4f1c-a776-f36daec1746b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Seller Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba12ae85-298f-454d-8a35-daddd3897a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save seller data\n",
    "silver_sell_directory = \"datamart/silver/sellers/\"\n",
    "if not os.path.exists(silver_sell_directory):\n",
    "    os.makedirs(silver_sell_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5bea31b9-621e-4630-8b17-1106edf023a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_sellers(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_sellers.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"seller_id\": StringType(),\n",
    "        \"seller_zip_code_prefix\": StringType(),\n",
    "        \"seller_city\": StringType(),\n",
    "        \"seller_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Check seller_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"seller_id\").distinct().count()\n",
    "    duplicates_seller_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'seller_id': {duplicates_seller_id}\")\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"seller_zip_code_prefix\",\n",
    "        F.lpad(col(\"seller_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_sellers.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd3ef983-9e83-4970-85fd-d7358d2f3de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: ../datamart/bronze/sellers/bronze_olist_sellers.parquet row count: 3095\n",
      "Number of duplicated 'seller_id': 0\n",
      "saved to: datamart/silver/sellers/silver_olist_sellers.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_sellers(\"../datamart/bronze/sellers/\",silver_sell_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1468622b-78f2-4fd7-a0b9-6c2e4b5a6cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: string (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "066674d2-760b-47fe-89ef-0be33600f1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5| 3095|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"seller_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73580bff-eb08-412b-b91e-59ed8767181a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Geolocation Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8548b10-ed7f-41d6-b767-f23f4c771a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save geolocation data\n",
    "silver_geo_directory = \"datamart/silver/geolocation/\"\n",
    "if not os.path.exists(silver_geo_directory):\n",
    "    os.makedirs(silver_geo_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7437f8bf-83cd-475d-a948-4bffd41e2fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_geolocation(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_geolocation.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"geolocation_zip_code_prefix\": StringType(),\n",
    "        \"geolocation_lat\": FloatType(),\n",
    "        \"geolocation_lng\": FloatType(),\n",
    "        \"geolocation_city\": StringType(),\n",
    "        \"geolocation_state\": StringType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Add missing leading zero\n",
    "    df = df.withColumn(\n",
    "        \"geolocation_zip_code_prefix\",\n",
    "        F.lpad(col(\"geolocation_zip_code_prefix\"), 5, \"0\")\n",
    "    )\n",
    "\n",
    "    # Deduplicate zipcodes by just taking the centroid (mean of lat,lng)\n",
    "    df_dedupe = df.groupBy(\"geolocation_zip_code_prefix\").agg(\n",
    "        F.avg(\"geolocation_lat\").alias(\"geolocation_lat\"),\n",
    "        F.avg(\"geolocation_lng\").alias(\"geolocation_lng\")\n",
    "    )\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_geolocation.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df_dedupe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "744285be-254a-482c-bbd4-f1b4dc73cef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: ../datamart/bronze/geolocation/bronze_olist_geolocation.parquet row count: 1000325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved to: datamart/silver/geolocation/silver_olist_geolocation.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_geolocation(\"../datamart/bronze/geolocation/\",silver_geo_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a145539-6949-49b3-bc73-f95e92a0191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geolocation_zip_code_prefix: string (nullable = true)\n",
      " |-- geolocation_lat: double (nullable = true)\n",
      " |-- geolocation_lng: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36a998b9-e0d1-46df-90f8-7ac1fc45901c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|length|count|\n",
      "+------+-----+\n",
      "|     5|19177|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check missing leading zero padded\n",
    "df.groupBy(F.length(\"geolocation_zip_code_prefix\").alias(\"length\")).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5857ffa-e899-4d38-a856-92d3a9c2a643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-----+\n",
      "|geolocation_zip_code_prefix|count|\n",
      "+---------------------------+-----+\n",
      "+---------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check every geolocation_zip_code_prefix only has 1 count. Group by prefix and count occurrences\n",
    "df.groupBy(\"geolocation_zip_code_prefix\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .filter(\"count > 1\") \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc538d2b-2b37-478b-844a-7af3724f1dcb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Products Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4909e9-3347-4b1f-9728-aba795fc18f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save products data\n",
    "silver_prod_directory = \"datamart/silver/products/\"\n",
    "if not os.path.exists(silver_prod_directory):\n",
    "    os.makedirs(silver_prod_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a42f5caf-d676-4975-b67c-203483680798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_products(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_products.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    # Rename columns due to spelling mistakes \n",
    "    df = df.withColumnRenamed(\"product_name_lenght\", \"product_name_length\") \\\n",
    "           .withColumnRenamed(\"product_description_lenght\", \"product_description_length\")\n",
    "\n",
    "    \n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"product_id\": StringType(),\n",
    "        \"product_category_name\": StringType(),\n",
    "        \"product_name_length\": DoubleType(),\n",
    "        \"product_description_length\": DoubleType(),\n",
    "        \"product_photos_qty\": DoubleType(),\n",
    "        \"product_weight_g\": DoubleType(),\n",
    "        \"product_length_cm\": DoubleType(),\n",
    "        \"product_height_cm\": DoubleType(),\n",
    "        \"product_width_cm\": DoubleType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Inputting missing values as NaN\n",
    "    df = df.fillna({\"product_category_name\": \"NaN\"})\n",
    "    df = df.fillna({\"product_name_length\": float('nan')}) \n",
    "    df = df.fillna({\"product_description_length\": float('nan')}) \n",
    "    df = df.fillna({\"product_photos_qty\": float('nan')}) \n",
    "    \n",
    "    # Check product_id duplicates (total rows - distinct ids)\n",
    "    total_rows = df.count()\n",
    "    distinct_rows = df.select(\"product_id\").distinct().count()\n",
    "    duplicates_product_id = total_rows - distinct_rows\n",
    "    print(f\"Number of duplicated 'product_id': {duplicates_product_id}\")\n",
    "\n",
    "\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_products.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "02873004-1348-4cf8-9323-cf9a4692b4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/products/bronze_olist_products.parquet row count: 32951\n",
      "Number of duplicated 'product_id': 0\n",
      "saved to: datamart/silver/products/silver_olist_products.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_products(\"datamart/bronze/products/\",silver_prod_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3864036-80cd-4e10-b1e8-9262992cd32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_category_name: string (nullable = false)\n",
      " |-- product_name_length: double (nullable = false)\n",
      " |-- product_description_length: double (nullable = false)\n",
      " |-- product_photos_qty: double (nullable = false)\n",
      " |-- product_weight_g: double (nullable = true)\n",
      " |-- product_length_cm: double (nullable = true)\n",
      " |-- product_height_cm: double (nullable = true)\n",
      " |-- product_width_cm: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62d0c758-2904-4604-83eb-9fa01659f789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|          product_id|product_category_name|product_name_length|product_description_length|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|1e9e8ef04dbcff454...|           perfumaria|               40.0|                     287.0|               1.0|           225.0|             16.0|             10.0|            14.0|\n",
      "|3aa071139cb16b67c...|                artes|               44.0|                     276.0|               1.0|          1000.0|             30.0|             18.0|            20.0|\n",
      "|96bd76ec8810374ed...|        esporte_lazer|               46.0|                     250.0|               1.0|           154.0|             18.0|              9.0|            15.0|\n",
      "|cef67bcfe19066a93...|                bebes|               27.0|                     261.0|               1.0|           371.0|             26.0|              4.0|            26.0|\n",
      "|9dc1a7de274444849...| utilidades_domest...|               37.0|                     402.0|               4.0|           625.0|             20.0|             17.0|            13.0|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df = spark.read.parquet(\"datamart/silver/products/silver_olist_products.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6e4638-eb87-4613-9279-a740b36a712a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Build Product Categories Table??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "761af31e-a1ce-48f7-b8c6-2458611a1704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not sure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cddad9d-b277-435c-bf4b-2cb863a93f98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fcfbe42-badc-42ee-9b92-bf2b9a665a79",
   "metadata": {},
   "source": [
    "### Build Orders Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dd19e8c-2ac8-4c03-9917-2c7073ef2205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save orders data\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "if not os.path.exists(silver_orders_directory):\n",
    "    os.makedirs(silver_orders_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "821674c0-c4ed-415b-96a5-fef4be96d167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_orders(bronze_directory, silver_directory, spark, partition_name):\n",
    "    filepath = os.path.join(bronze_directory, partition_name)\n",
    "    df = spark.read.option(\"header\", True).option(\"inferSchema\", True).csv(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "\n",
    "# def process_silver_olist_orders(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    \n",
    "    # partition_name = \"bronze_olist_orders.parquet\"\n",
    "    # filepath = bronze_directory + partition_name\n",
    "    # df = spark.read.parquet(filepath)\n",
    "    # print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "\n",
    "\n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"order_id\": StringType(),\n",
    "        \"customer_id\": StringType(),\n",
    "        \"order_status\": StringType(),\n",
    "        \"order_purchase_timestamp\": TimestampType(),\n",
    "        \"order_approved_at\": TimestampType(),\n",
    "        \"order_delivered_carrier_date\": TimestampType(),\n",
    "        \"order_delivered_customer_date\": TimestampType(),\n",
    "        \"order_estimated_delivery_date\": TimestampType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    # Removing Invalid order ids\n",
    "    # Load the Bronze table \n",
    "    df_order_items = spark.read.parquet(\"datamart/bronze/order_items/bronze_olist_order_items.parquet\") \n",
    "    \n",
    "    # Get distinct order IDs that exist in order items\n",
    "    valid_order_ids_df = df_order_items.select(\"order_id\").distinct()\n",
    "    \n",
    "    \n",
    "    # Keep only orders that exist in df_order_items\n",
    "    df_orders_clean = df.join(valid_order_ids_df, on=\"order_id\", how=\"inner\")\n",
    "    \n",
    "    # Count how many were dropped\n",
    "    dropped_orders = df.count() - df_orders_clean.count()\n",
    "    print(f\"Dropped {dropped_orders} orders with no items.\")\n",
    "\n",
    "    df = df_orders_clean\n",
    "\n",
    "\n",
    "    # Checking for invalid customer IDs\n",
    "    # Load df_customers from Bronze\n",
    "    df_customers = spark.read.parquet(\"datamart/bronze/customers/bronze_olist_customers.parquet\")  \n",
    "\n",
    "    # Get distinct valid customer IDs\n",
    "    valid_customer_ids_df = df_customers.select(\"customer_id\").distinct()\n",
    "    \n",
    "    # Perform a left anti join to find orders with invalid customer_id\n",
    "    invalid_orders = df.join(valid_customer_ids_df, on=\"customer_id\", how=\"left_anti\")\n",
    "    \n",
    "    # Count how many invalid customer IDs there are\n",
    "    invalid_customer_count = invalid_orders.count()\n",
    "\n",
    "    # Conditionally drop invalid orders\n",
    "    if invalid_customer_count > 0:\n",
    "        initial_count = df.count()\n",
    "        print(\"Dropping orders with invalid customer_id...\")\n",
    "        df = df.join(valid_customer_ids_df, on=\"customer_id\", how=\"inner\")\n",
    "        final_count = df.count()\n",
    "        dropped_count = initial_count - final_count\n",
    "        print(f\"Dropped {dropped_count} rows\")\n",
    "        \n",
    "    else:\n",
    "        print(\"All customer ids are valid — no need to drop!!\")\n",
    "\n",
    "\n",
    "\n",
    "    # Enforcing enum for order statuses\n",
    "    # Define valid statuses \n",
    "    valid_statuses = {\n",
    "        \"created\",\n",
    "        \"approved\",\n",
    "        \"processing\",\n",
    "        \"invoiced\",\n",
    "        \"shipped\",\n",
    "        \"delivered\",\n",
    "        \"canceled\",\n",
    "        \"unavailable\"\n",
    "    }\n",
    "    \n",
    "    # Clean and standardize the `order_status` column\n",
    "    df = df.withColumn(\"order_status\", trim(lower(col(\"order_status\"))))\n",
    "    \n",
    "    # dentify invalid statuses (those NOT in the valid_statuses set)\n",
    "    invalid_statuses_df = df.filter(~col(\"order_status\").isin(list(valid_statuses)))\n",
    "    \n",
    "    # Print the unique invalid statuses\n",
    "    invalid_statuses_list = invalid_statuses_df.select(\"order_status\").distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "    if invalid_statuses_list:\n",
    "        print(f\"Invalid statuses found: {invalid_statuses_list}\")\n",
    "    else:\n",
    "        print(\"No invalid status found!!\")\n",
    "\n",
    "\n",
    "    \n",
    "    # # save silver table - IRL connect to database to write\n",
    "    # partition_name = \"silver_olist_orders_2016_09.parquet\"  \n",
    "    # filepath = silver_directory + partition_name\n",
    "    # df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    # print('saved to:', filepath)\n",
    "\n",
    "\n",
    "    # save using partition_name but replace `.csv` with `.parquet`\n",
    "    parquet_name = partition_name.replace(\".csv\", \".parquet\")\n",
    "    output_path = os.path.join(silver_directory, parquet_name)\n",
    "    df.write.mode(\"overwrite\").parquet(output_path)\n",
    "    print(\"-----> saved to:\", output_path)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1c38f543-33b9-4ae0-ab4c-c34776d9ea89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Processing bronze_olist_orders_2016_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_09.csv row count: 4\n",
      "Dropped 1 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2016_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2016_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_10.csv row count: 324\n",
      "Dropped 16 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2016_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2016_12.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2016_12.csv row count: 1\n",
      "Dropped 0 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2016_12.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_01.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_01.csv row count: 800\n",
      "Dropped 11 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_01.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_02.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_02.csv row count: 1780\n",
      "Dropped 47 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_02.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_03.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_03.csv row count: 2682\n",
      "Dropped 41 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_03.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_04.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_04.csv row count: 2404\n",
      "Dropped 13 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_04.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_05.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_05.csv row count: 3700\n",
      "Dropped 40 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_05.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_06.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_06.csv row count: 3245\n",
      "Dropped 28 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_06.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_07.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_07.csv row count: 4026\n",
      "Dropped 57 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_07.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_08.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_08.csv row count: 4331\n",
      "Dropped 38 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_08.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_09.csv row count: 4285\n",
      "Dropped 42 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_10.csv row count: 4631\n",
      "Dropped 63 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_11.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_11.csv row count: 7544\n",
      "Dropped 93 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_11.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2017_12.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2017_12.csv row count: 5673\n",
      "Dropped 49 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2017_12.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_01.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_01.csv row count: 7269\n",
      "Dropped 49 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_01.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_02.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_02.csv row count: 6728\n",
      "Dropped 34 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_02.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_03.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_03.csv row count: 7211\n",
      "Dropped 23 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_03.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_04.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_04.csv row count: 6939\n",
      "Dropped 5 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_04.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_05.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_05.csv row count: 6873\n",
      "Dropped 20 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_05.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_06.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_06.csv row count: 6167\n",
      "Dropped 7 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_06.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_07.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_07.csv row count: 6292\n",
      "Dropped 19 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_07.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_08.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_08.csv row count: 6512\n",
      "Dropped 60 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_08.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_09.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_09.csv row count: 16\n",
      "Dropped 15 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_09.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n",
      "\n",
      "======== Processing bronze_olist_orders_2018_10.csv ......... \n",
      "\n",
      "loaded from: datamart/bronze/orders/bronze_olist_orders_2018_10.csv row count: 4\n",
      "Dropped 4 orders with no items.\n",
      "All customer ids are valid — no need to drop!!\n",
      "No invalid status found!!\n",
      "-----> saved to: datamart/silver/orders/bronze_olist_orders_2018_10.parquet\n",
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      " |-- snapshot_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "\n",
    "# Set base directory\n",
    "bronze_orders_directory = \"datamart/bronze/orders/\"\n",
    "silver_orders_directory = \"datamart/silver/orders/\"\n",
    "\n",
    "# List all CSV files in the bronze orders folder\n",
    "csv_files = [f for f in os.listdir(bronze_orders_directory) if f.endswith(\".csv\")]\n",
    "\n",
    "# Sort the files according to date\n",
    "csv_files.sort()\n",
    "\n",
    "# Loop through each file and process\n",
    "for partition_name in csv_files:\n",
    "    print(f\"\\n======== Processing {partition_name} ......... \\n\")\n",
    "    df = process_silver_olist_orders(bronze_orders_directory, silver_orders_directory, spark, partition_name)\n",
    "    \n",
    "    # Check schema enforced\n",
    "    df.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57dda5c-b11c-4bcc-833c-89b9158b35f7",
   "metadata": {},
   "source": [
    "### Build Order_Items Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "950f9874-fa4c-47c1-823e-d9e6d21104e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create silver directory to save order_items data\n",
    "silver_order_items_directory = \"datamart/silver/order_items/\"\n",
    "if not os.path.exists(silver_order_items_directory):\n",
    "    os.makedirs(silver_order_items_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a05472f-0707-49c9-a85a-7a8addb30c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_silver_olist_order_items(bronze_directory, silver_directory, spark):\n",
    "    \n",
    "    # connect to bronze table\n",
    "    partition_name = \"bronze_olist_order_items.parquet\"\n",
    "    filepath = bronze_directory + partition_name\n",
    "    df = spark.read.parquet(filepath)\n",
    "    print('loaded from:', filepath, 'row count:', df.count())\n",
    "\n",
    "    \n",
    "    # clean data: enforce schema / data type\n",
    "    # Dictionary specifying columns and their desired datatypes\n",
    "    column_type_map = {\n",
    "        \"order_id\": StringType(),\n",
    "        \"order_item_id\": LongType(),\n",
    "        \"product_id\": StringType(),\n",
    "        \"seller_id\": StringType(),\n",
    "        \"shipping_limit_date\": TimestampType(),\n",
    "        \"price\": DoubleType(),\n",
    "        \"freight_value\": DoubleType(),\n",
    "    }\n",
    "\n",
    "    for column, new_type in column_type_map.items():\n",
    "        df = df.withColumn(column, col(column).cast(new_type))\n",
    "\n",
    "    \n",
    "    # Checking for invalid seller IDs\n",
    "    # Load df_sellers from Bronze\n",
    "    df_sellers = spark.read.parquet(\"datamart/bronze/sellers/bronze_olist_sellers.parquet\")  \n",
    "\n",
    "    # Get distinct valid seller IDs\n",
    "    valid_seller_ids_df = df_sellers.select(\"seller_id\").distinct()\n",
    "    \n",
    "    # Perform a left anti join to find sellers with invalid seller_id\n",
    "    invalid_orders = df.join(valid_seller_ids_df, on=\"seller_id\", how=\"left_anti\")\n",
    "    \n",
    "    # Count how many invalid seller IDs there are\n",
    "    invalid_seller_count = invalid_orders.count()\n",
    "\n",
    "    # Conditionally drop invalid orders\n",
    "    if invalid_seller_count > 0:\n",
    "        initial_count = df.count()\n",
    "        print(\"Dropping orders with invalid seller_id...\")\n",
    "        df = df.join(valid_seller_ids_df, on=\"seller_id\", how=\"inner\")\n",
    "        final_count = df.count()\n",
    "        dropped_count = initial_count - final_count\n",
    "        print(f\"Dropped {dropped_count} rows\")\n",
    "        \n",
    "    else:\n",
    "        print(\"All seller ids are valid — no need to drop!!\")\n",
    "\n",
    "    \n",
    "    # save silver table - IRL connect to database to write\n",
    "    partition_name = \"silver_olist_order_items.parquet\"\n",
    "    filepath = silver_directory + partition_name\n",
    "    df.write.mode(\"overwrite\").parquet(filepath)\n",
    "    print('saved to:', filepath)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b7b5663-92f0-426e-b6a8-9e7faa45d77d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded from: datamart/bronze/order_items/bronze_olist_order_items.parquet row count: 112650\n",
      "All seller ids are valid — no need to drop!!\n",
      "saved to: datamart/silver/order_items/silver_olist_order_items.parquet\n"
     ]
    }
   ],
   "source": [
    "# Run function manually to test\n",
    "# I inputted the bronze_directory manually (amend after our path discrepancies are resolved)\n",
    "df = process_silver_olist_order_items(\"datamart/bronze/order_items/\",silver_order_items_directory, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff299665-0d75-468e-acc7-15fd8f412f34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: long (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check schema enforced\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "97a8e4aa-0994-4877-9981-a7927e3febbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date| price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "|89bed55dd88d035e3...|            1|ae6b739ab6e9d7991...|53e4c6e0f4312d4d2...|2018-01-11 17:12:22|  42.0|         17.6|\n",
      "|89c037e2b749a2ed5...|            1|e906fa76a27488f80...|1835b56ce799e6a4d...|2017-11-30 04:15:33| 53.99|        12.72|\n",
      "|89c037e2b749a2ed5...|            2|e906fa76a27488f80...|1835b56ce799e6a4d...|2017-11-30 04:15:33| 53.99|        12.72|\n",
      "|89c04d22504649482...|            1|3d73c88390adac7dd...|c8b0e2b0a7095e5d8...|2018-03-14 00:20:26|199.99|        25.05|\n",
      "|89c0bf5292a493fb2...|            1|2d40d83fc97b8d4d4...|cca3071e3e9bb7d12...|2017-08-24 03:26:15|  29.9|        11.85|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+------+-------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Inspect some output\n",
    "df = spark.read.parquet(\"datamart/silver/order_items/silver_olist_order_items.parquet\")\n",
    "df.show(5)\n",
    "\n",
    "# Can read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76752735-5f9b-42f2-8950-e1283efabee0",
   "metadata": {},
   "source": [
    "## Build Gold Table (Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cb3e0-1fe4-48f0-9a46-0f300154ff8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c818d8e7-a22a-40ab-abdf-ace25c516543",
   "metadata": {},
   "source": [
    "## Inspect Feature Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a7d963-2611-4896-be4a-7fbbd641f4ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5a6b17-8978-405a-8b04-6c6f4c7a3e52",
   "metadata": {},
   "source": [
    "## Build Gold Table (Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b200137d-ca20-42b9-a3df-0b662024a899",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "70eeb8a3-7737-4556-a85c-e844b47f6454",
   "metadata": {},
   "source": [
    "## Inspect Label Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc20833e-9653-4c4c-9ce0-0023cca719b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "974a0dd8-3602-42b8-b429-86cb7a2fe872",
   "metadata": {},
   "source": [
    "## Stop Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97a33e6-9799-4a59-9588-40af6b036604",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# End spark session\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mspark\u001b[49m.stop()\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m---completed job---\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "# End spark session\n",
    "spark.stop()\n",
    "\n",
    "print('\\n\\n---completed job---\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
